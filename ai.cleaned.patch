(cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.ymlnew file mode 100644index 0000000000000000000000000000000000000000..c106a9a5eff39131a36a43f6e8d149da0aa3df58--- /dev/null+++ b/.github/workflows/ci.yml@@ -0,0 +1,37 @@+name: CI++on:+  push:+  pull_request:++jobs:+  build:+    runs-on: ubuntu-latest+    steps:+      - uses: actions/checkout@v4+      - name: Set up Python+        uses: actions/setup-python@v5+        with:+          python-version: "3.11"+      - name: Install dependencies+        run: |+          python -m pip install --upgrade pip+          pip install -e .[dev]+      - name: Ruff+        run: ruff check .+      - name: Black+        run: black --check .+      - name: Mypy+        run: mypy .+      - name: Pytest+        run: pytest+      - name: Verify Atlas+        run: python tools/verifiers/Verify-Atlas.py+      - name: Verify Brains+        run: python tools/verifiers/Verify-Brains.py+      - name: Verify Patch Notes+        run: python tools/verifiers/Verify-PatchNotes.py+      - name: Verify Handoff+        run: python tools/verifiers/Verify-Handoff.py+      - name: Verify UI Contract+        run: python tools/verifiers/Verify-UI-Contract.pydiff --git a/.gitignore b/.gitignorenew file mode 100644index 0000000000000000000000000000000000000000..52a3aa9911763e9ad7d8cb656cdc33d506ee2b5b--- /dev/null+++ b/.gitignore@@ -0,0 +1,17 @@+__pycache__/+*.py[cod]+.mypy_cache/+.pytest_cache/+.ruff_cache/+*.egg-info/+.eggs/+*.coverage+coverage.xml+htmlcov/+.DS_Store+*.swp+.venv/+.env/++# Streamlit state caches+.streamlit/diff --git a/LICENSE b/LICENSEnew file mode 100644index 0000000000000000000000000000000000000000..7154e608ef9339f76af05fcde2571c601ace5d90--- /dev/null+++ b/LICENSE@@ -0,0 +1,21 @@+MIT License++Copyright (c) 2024 Spectra App Contributors++Permission is hereby granted, free of charge, to any person obtaining a copy+of this software and associated documentation files (the "Software"), to deal+in the Software without restriction, including without limitation the rights+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell+copies of the Software, and to permit persons to whom the Software is+furnished to do so, subject to the following conditions:++The above copyright notice and this permission notice shall be included in all+copies or substantial portions of the Software.++THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE+SOFTWARE.diff --git a/PATCH_NOTES/PATCH_NOTES_v0.1.0(a).md b/PATCH_NOTES/PATCH_NOTES_v0.1.0(a).mdnew file mode 100644index 0000000000000000000000000000000000000000..1217cfabad056b673a872f255901458d8ec37dd6--- /dev/null+++ b/PATCH_NOTES/PATCH_NOTES_v0.1.0(a).md@@ -0,0 +1,23 @@+# Spectra App v0.1.0 (a) — Bootstrap++## Highlights+- First runnable Streamlit interface with Overlay, Differential, Star Hub, and Docs tabs.+- Canonical ingestion pipeline for ASCII spectra with provenance logging and deduplication.+- Export bundle delivering manifest v2, per-trace CSVs, and optional PNG snapshot.+- SIMBAD resolver integration (with offline fallback) and Fe I line overlay catalogue.++## Changes+- Implemented session state, unit toggles (nm/Å/µm/cm⁻¹), and line overlay controls.+- Added numerical engines for wavelength/unit transforms, resolution matching, and differential analyses.+- Populated `/tools/verifiers` and CI workflow to enforce atlas/brains/notes/handoff updates and UI contract.+- Seeded example data, docs placeholders, and atlas documentation for every subsystem.++## Known Issues+- FITS ingestion and external archive adapters (MAST/SDSS) are stubbed pending future runs.+- Export replay helper is limited to canonical spectra; UI reconstruction will be expanded.++## Verification+- `ruff check . --fix`+- `black .`+- `mypy .`+- `pytest`diff --git a/PATCH_NOTES/PATCH_NOTES_v0.1.0(b).md b/PATCH_NOTES/PATCH_NOTES_v0.1.0(b).mdnew file mode 100644index 0000000000000000000000000000000000000000..084ff43eedb592da7ff6223f822e926b63cb4292--- /dev/null+++ b/PATCH_NOTES/PATCH_NOTES_v0.1.0(b).md@@ -0,0 +1,33 @@+# Spectra App v0.1.0 (b) — FITS ingestion++## Highlights+- Added FITS ingestion with header/WCS parsing, uncertainty support, and provenance logging.+- Extended canonicalisation to normalise FITS spectra onto the vacuum-nm baseline.+- Overlay tab uploader now accepts FITS files alongside ASCII uploads.++## Changes+- Implemented `server/ingest/fits_loader.py` with metadata harvesting, SHA-256 dedupe hash, and+  WCS-aware wavelength grids.+- Introduced `canonicalize_fits` plus reusable wavelength-unit normalisation in+  `server/ingest/canonicalize.py`.+- Updated the Streamlit overlay flow to route FITS files through the new ingest path and surface+  mixed-type uploads without duplicates.+- Added regression tests for FITS ingestion and air↔vacuum conversion; seeded a sample FITS file in+  `data/examples/`.+- Refreshed documentation/atlas entries and version metadata for v0.1.0b.++## Known Issues+- Archive fetchers (MAST/SDSS) remain stubbed; Star Hub lists resolver results only.+- Replay script still limited to canonical spectra reconstruction; UI rebuild CLI pending.+- Performance optimisations (async ingest, LOD) are outstanding.++## Verification+- `ruff check . --fix`+- `black .`+- `mypy .`+- `pytest`+- `python tools/verifiers/Verify-Atlas.py`+- `python tools/verifiers/Verify-Brains.py`+- `python tools/verifiers/Verify-PatchNotes.py`+- `python tools/verifiers/Verify-Handoff.py`+- `python tools/verifiers/Verify-UI-Contract.py`diff --git a/README.md b/README.mdindex a564aecef783e96ca9b50c989bae07c9355d2f08..5ade68bd11d7aa4179a89967f360e1593264fc06 100644--- a/README.md+++ b/README.md@@ -1,2 +1,38 @@-# spectrasuite-Trying to test out an Ai hand off only version, where it recreates the environment from scratch and i dont have to do any prompt hand offs.+# Spectra App++Spectra App is a research-grade Streamlit application for comparing uploaded spectra against+archival references with reproducible provenance. This repository follows the "Spectra App —+Fresh Start" master brief and ships with:++- A canonical wavelength baseline (vacuum nanometers) with reversible unit toggles.+- Robust ASCII and FITS ingestion with provenance logging and deduplication.+- Export bundles that reproduce the visible state via a manifest (schema v2).+- Continuity artifacts in `/atlas`, `/brains`, `/PATCH_NOTES`, and `/handoffs` to keep every+  run auditable.++## Getting Started++```bash+python -m venv .venv+source .venv/bin/activate+pip install -e .[dev]+streamlit run app/app_patched.py+```++## Testing & Quality Gates++```bash+ruff check .+black --check .+mypy .+pytest+```++## Documentation++- Atlas: system design notes per subsystem (`/atlas`).+- Brains: run journal (`/brains`).+- Patch notes: user-facing change log (`/PATCH_NOTES`).+- Handoffs: cross-run context (`/handoffs`).++The Docs tab inside the app renders the content from `docs/static/`.diff --git a/app/__init__.py b/app/__init__.pynew file mode 100644index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391diff --git a/app/app_patched.py b/app/app_patched.pynew file mode 100644index 0000000000000000000000000000000000000000..86b4761735c17c0cf74d857f2514e25770808ba6--- /dev/null+++ b/app/app_patched.py@@ -0,0 +1,8 @@+"""Streamlit entry point used by Streamlit CLI."""++from __future__ import annotations++from app.ui.main import run_app++if __name__ == "__main__":+    run_app()diff --git a/app/config/settings.yaml b/app/config/settings.yamlnew file mode 100644index 0000000000000000000000000000000000000000..44ffb4762f968b096652d7e361864718628481c4--- /dev/null+++ b/app/config/settings.yaml@@ -0,0 +1,17 @@+ui:+  default_display_mode: flux_density+  default_wavelength_unit: nm+  duplicate_scope: session+  max_visible_traces: 12+  allow_line_overlays: true+limits:+  max_upload_mb: 10+  max_points: 500000+  network_timeout_s: 15+line_overlays:+  default_species: Fe I+  default_scaling: relative+  default_gamma: 0.85+export:+  manifest_schema_version: 2+  include_png: truediff --git a/app/config/version.json b/app/config/version.jsonnew file mode 100644index 0000000000000000000000000000000000000000..51d2185abd89df63d5eb2b6c84faad6cce6e8cc1--- /dev/null+++ b/app/config/version.json@@ -0,0 +1,4 @@+{+  "app_version": "0.1.0b",+  "schema_version": 2+}diff --git a/app/state/__init__.py b/app/state/__init__.pynew file mode 100644index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391diff --git a/app/state/session.py b/app/state/session.pynew file mode 100644index 0000000000000000000000000000000000000000..e19f58231d016e5c0e5cb06a6ae08304caba4bb1--- /dev/null+++ b/app/state/session.py@@ -0,0 +1,141 @@+"""Application session state and helpers."""++from __future__ import annotations++from collections.abc import Iterable+from dataclasses import dataclass, field+from enum import Enum++from server.models import CanonicalSpectrum+++class XAxisUnit(str, Enum):+    """Supported wavelength display units."""++    NM = "nm"+    ANGSTROM = "angstrom"+    MICRON = "micron"+    WAVENUMBER = "wavenumber"+++class DisplayMode(str, Enum):+    """Supported y-axis display modes."""++    FLUX_DENSITY = "flux_density"+    TRANSMISSION = "transmission"+    ABSORBANCE = "absorbance"+    OPTICAL_DEPTH = "optical_depth"+    RELATIVE_INTENSITY = "relative_intensity"+++@dataclass(slots=True)+class TraceView:+    """Visibility and organization metadata for a trace."""++    trace_id: str+    is_visible: bool = True+    is_pinned: bool = False+    is_derived: bool = False+++@dataclass(slots=True)+class AppSessionState:+    """Session-scoped state stored inside Streamlit's session_state."""++    traces: dict[str, CanonicalSpectrum] = field(default_factory=dict)+    trace_views: dict[str, TraceView] = field(default_factory=dict)+    trace_order: list[str] = field(default_factory=list)+    x_axis_unit: XAxisUnit = XAxisUnit.NM+    display_mode: DisplayMode = DisplayMode.FLUX_DENSITY+    duplicate_scope: str = "session"+    ingest_ledger: set[tuple[str | None, str]] = field(default_factory=set)++    def register_trace(+        self, trace: CanonicalSpectrum, *, allow_duplicates: bool = False, is_derived: bool = False+    ) -> tuple[bool, str]:+        """Add a trace if it is not a duplicate. Returns (added?, trace_id)."""++        signature = (trace.source_hash, trace.metadata.product_id or trace.label)+        if not allow_duplicates and signature in self.ingest_ledger:+            existing_id = self._find_trace_by_signature(signature)+            return False, existing_id or trace.label++        trace_id = self._next_trace_id(trace.label)+        self.traces[trace_id] = trace+        self.trace_views[trace_id] = TraceView(trace_id=trace_id, is_derived=is_derived)+        self.trace_order.append(trace_id)+        self.ingest_ledger.add(signature)+        return True, trace_id++    def _find_trace_by_signature(self, signature: tuple[str | None, str]) -> str | None:+        for trace_id, trace in self.traces.items():+            candidate = (trace.source_hash, trace.metadata.product_id or trace.label)+            if candidate == signature:+                return trace_id+        return None++    def _next_trace_id(self, label: str) -> str:+        base = label.replace(" ", "_").lower() or "trace"+        candidate = base+        suffix = 1+        while candidate in self.traces:+            suffix += 1+            candidate = f"{base}_{suffix}"+        return candidate++    def visible_traces(self) -> list[CanonicalSpectrum]:+        ordered: list[CanonicalSpectrum] = []+        for trace_id in self.trace_order:+            view = self.trace_views.get(trace_id)+            if view and view.is_visible:+                ordered.append(self.traces[trace_id])+        return ordered++    def toggle_visibility(self, trace_id: str, visible: bool) -> None:+        if trace_id in self.trace_views:+            self.trace_views[trace_id].is_visible = visible++    def remove_trace(self, trace_id: str) -> None:+        if trace_id in self.traces:+            trace = self.traces[trace_id]+            signature = (trace.source_hash, trace.metadata.product_id or trace.label)+            self.ingest_ledger.discard(signature)+            del self.traces[trace_id]+            self.trace_order = [tid for tid in self.trace_order if tid != trace_id]+            self.trace_views.pop(trace_id, None)++    def set_axis_unit(self, unit: XAxisUnit) -> None:+        self.x_axis_unit = unit++    def set_display_mode(self, mode: DisplayMode) -> None:+        self.display_mode = mode++    def iter_traces(self) -> Iterable[tuple[str, CanonicalSpectrum]]:+        for trace_id in self.trace_order:+            yield trace_id, self.traces[trace_id]+++SESSION_STATE_KEY = "spectra_app_session"+++def get_session_state(st_module, *, default: AppSessionState | None = None) -> AppSessionState:+    """Retrieve or initialize the session state from Streamlit."""++    if SESSION_STATE_KEY not in st_module.session_state:+        st_module.session_state[SESSION_STATE_KEY] = default or AppSessionState()+    return st_module.session_state[SESSION_STATE_KEY]+++def reset_session_state(st_module) -> None:+    st_module.session_state.pop(SESSION_STATE_KEY, None)+++__all__ = [+    "AppSessionState",+    "DisplayMode",+    "SESSION_STATE_KEY",+    "TraceView",+    "XAxisUnit",+    "get_session_state",+    "reset_session_state",+]diff --git a/app/ui/__init__.py b/app/ui/__init__.pynew file mode 100644index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391diff --git a/app/ui/components/__init__.py b/app/ui/components/__init__.pynew file mode 100644index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391diff --git a/app/ui/differential.py b/app/ui/differential.pynew file mode 100644index 0000000000000000000000000000000000000000..0ddc5c447d4a1c5934a646424d5a4745f3e88f66--- /dev/null+++ b/app/ui/differential.py@@ -0,0 +1,105 @@+"""Differential tab UI."""++from __future__ import annotations++from dataclasses import dataclass++import numpy as np+import streamlit as st++from app.state.session import AppSessionState+from server.math.differential import DifferentialProduct, divide, subtract+from server.models import CanonicalSpectrum, ProvenanceEvent+++@dataclass(slots=True)+class DifferentialSettings:+    epsilon: float = 1e-8+++def _add_trace(session: AppSessionState, product: DifferentialProduct, *, epsilon: float) -> None:+    spectrum = product.spectrum+    spectrum.provenance.append(+        ProvenanceEvent(+            step="differential_ui_add",+            parameters={"epsilon": epsilon, "operation": product.operation},+        )+    )+    session.register_trace(spectrum, allow_duplicates=True, is_derived=True)+++def _add_trivial_trace(session: AppSessionState, base: CanonicalSpectrum, label: str) -> None:+    zeros = np.zeros_like(base.values)+    spectrum = CanonicalSpectrum(+        label=label,+        wavelength_vac_nm=base.wavelength_vac_nm,+        values=zeros,+        value_mode=base.value_mode,+        value_unit=base.value_unit,+        metadata=base.metadata,+        provenance=base.provenance + [ProvenanceEvent(step="differential_trivial", parameters={})],+        source_hash=None,+        uncertainties=np.zeros_like(base.values) if base.uncertainties is not None else None,+    )+    session.register_trace(spectrum, allow_duplicates=True, is_derived=True)+++def render_differential_tab(+    session: AppSessionState, settings: DifferentialSettings | None = None+) -> None:+    settings = settings or DifferentialSettings()+    st.subheader("Differential Analysis")++    if len(session.trace_order) < 2:+        st.info("Add at least two traces to compute differentials.")+        return++    def _format(trace_id: str) -> str:+        return session.traces[trace_id].label++    a_id = st.selectbox("Trace A", session.trace_order, format_func=_format)+    b_id = st.selectbox(+        "Trace B",+        session.trace_order,+        index=min(1, len(session.trace_order) - 1),+        format_func=_format,+    )++    trace_a = session.traces[a_id]+    trace_b = session.traces[b_id]++    col1, col2 = st.columns(2)+    with col1:+        if st.button("Compute A - B"):+            product = subtract(trace_a, trace_b, epsilon=settings.epsilon)+            if product is None:+                st.warning("Traces are numerically identical; difference suppressed.")+                if st.button("Add zero difference anyway", key="add_diff_anyway"):+                    _add_trivial_trace(session, trace_a, f"{trace_a.label}-{trace_b.label}")+            else:+                _add_trace(session, product, epsilon=settings.epsilon)+                st.success(f"Added differential trace {product.spectrum.label}")+    with col2:+        if st.button("Compute A / B"):+            product = divide(trace_a, trace_b, epsilon=settings.epsilon)+            if product is None:+                st.warning("Traces are numerically identical; ratio suppressed.")+                if st.button("Add unity ratio anyway", key="add_ratio_anyway"):+                    unity = CanonicalSpectrum(+                        label=f"{trace_a.label}/{trace_b.label}",+                        wavelength_vac_nm=trace_a.wavelength_vac_nm,+                        values=np.ones_like(trace_a.values),+                        value_mode="relative_intensity",+                        value_unit=None,+                        metadata=trace_a.metadata,+                        provenance=trace_a.provenance+                        + [ProvenanceEvent(step="differential_trivial", parameters={})],+                        source_hash=None,+                        uncertainties=None,+                    )+                    session.register_trace(unity, allow_duplicates=True, is_derived=True)+            else:+                _add_trace(session, product, epsilon=settings.epsilon)+                st.success(f"Added differential trace {product.spectrum.label}")++    st.caption("Differentials respect flux-conserving resampling with ε-stabilized ratios.")diff --git a/app/ui/docs.py b/app/ui/docs.pynew file mode 100644index 0000000000000000000000000000000000000000..f30952448d36a9bd82db4d576400b0d7ff140c34--- /dev/null+++ b/app/ui/docs.py@@ -0,0 +1,21 @@+"""Docs tab renderer."""++from __future__ import annotations++from collections.abc import Iterable+from pathlib import Path++import streamlit as st++_DOCS_DIR = Path(__file__).resolve().parents[2] / "docs" / "static"+++def _load_docs() -> Iterable[Path]:+    return sorted(path for path in _DOCS_DIR.glob("*.md"))+++def render_docs_tab() -> None:+    st.header("Documentation")+    for path in _load_docs():+        st.subheader(path.stem.replace("_", " ").title())+        st.markdown(path.read_text(encoding="utf-8"))diff --git a/app/ui/main.py b/app/ui/main.pynew file mode 100644index 0000000000000000000000000000000000000000..8c8d17e3a3f487edd73943257ba2d3f25f639005--- /dev/null+++ b/app/ui/main.py@@ -0,0 +1,177 @@+"""Main Streamlit entry point."""++from __future__ import annotations++import json+from dataclasses import dataclass+from pathlib import Path+from typing import Any++import streamlit as st+import yaml++from app.state.session import AppSessionState, DisplayMode, XAxisUnit, get_session_state+from app.ui.differential import render_differential_tab+from app.ui.docs import render_docs_tab+from app.ui.overlay import LineOverlaySettings, OverlayRenderResult, render_overlay_tab+from app.ui.star_hub import render_star_hub_tab+from server.export.manifest import export_session+from server.fetchers import resolver_simbad  # noqa: F401 - ensure module discovery+from server.overlays.lines import LineCatalog+++@dataclass(slots=True)+class UIContract:+    tabs: list[str]+    sidebar_sections: list[str]+    version_badge_label: str+++@dataclass(slots=True)+class AppConfig:+    app_version: str+    schema_version: int+    settings: dict[str, Any]+++def load_config() -> AppConfig:+    root = Path(__file__).resolve().parents[2]+    version_payload = json.loads((root / "app" / "config" / "version.json").read_text())+    settings_payload = yaml.safe_load((root / "app" / "config" / "settings.yaml").read_text())+    return AppConfig(+        app_version=version_payload["app_version"],+        schema_version=int(version_payload["schema_version"]),+        settings=settings_payload,+    )+++def get_ui_contract() -> UIContract:+    return UIContract(+        tabs=["Overlay", "Differential", "Star Hub", "Docs"],+        sidebar_sections=[+            "Examples",+            "Display Mode",+            "Units",+            "Duplicate Scope",+            "Line Overlays",+        ],+        version_badge_label="Version",+    )+++def _configure_sidebar(+    session: AppSessionState, catalog: LineCatalog, settings: dict[str, Any]+) -> LineOverlaySettings:+    st.sidebar.header("Examples")+    st.sidebar.write("Load bundled examples from data/examples in future runs.")++    st.sidebar.header("Display Mode")+    display_choice = st.sidebar.radio(+        "Y-axis mode",+        options=[DisplayMode.FLUX_DENSITY.value, DisplayMode.RELATIVE_INTENSITY.value],+        index=0 if session.display_mode == DisplayMode.FLUX_DENSITY else 1,+        key="display_mode_radio",+    )+    session.set_display_mode(DisplayMode(display_choice))++    st.sidebar.header("Units")+    axis_unit = st.sidebar.radio(+        "Wavelength units",+        options=[unit.value for unit in XAxisUnit],+        index=list(XAxisUnit).index(session.x_axis_unit),+        key="axis_unit_radio",+    )+    session.set_axis_unit(XAxisUnit(axis_unit))++    st.sidebar.header("Duplicate Scope")+    session.duplicate_scope = st.sidebar.selectbox(+        "Duplicate policy",+        options=["session", "global"],+        index=0,+        key="duplicate_scope_select",+    )++    st.sidebar.header("Line Overlays")+    species_options = catalog.species()+    default_species = settings.get("line_overlays", {}).get("default_species")+    species_index = (+        species_options.index(default_species) if default_species in species_options else 0+    )+    species = st.sidebar.selectbox("Species", species_options, index=species_index)+    mode = st.sidebar.selectbox("Scaling mode", ["relative", "quantile"])+    gamma = st.sidebar.slider("Gamma", min_value=0.6, max_value=1.0, value=0.85, step=0.05)+    threshold = st.sidebar.slider(+        "Relative threshold", min_value=0.0, max_value=1.0, value=0.0, step=0.05+    )+    velocity = st.sidebar.slider(+        "Δv (km/s)", min_value=-300.0, max_value=300.0, value=0.0, step=5.0+    )++    return LineOverlaySettings(+        species=species, mode=mode, gamma=gamma, threshold=threshold, velocity_kms=velocity+    )+++def _header(app_version: str) -> None:+    st.title("Spectra App")+    st.caption("Research-grade spectral comparison toolkit")+    st.markdown(f"**Version:** `{app_version}`")+    st.text_input("Global search", placeholder="Search traces or metadata", key="global_search")+++def run_app() -> None:+    config = load_config()+    st.set_page_config(page_title="Spectra App", layout="wide")+    session = get_session_state(st, default=AppSessionState())+    catalog = LineCatalog()++    _header(config.app_version)++    line_settings = _configure_sidebar(session, catalog, config.settings)++    overlay_tab, differential_tab, star_hub_tab, docs_tab = st.tabs(get_ui_contract().tabs)++    export_result: OverlayRenderResult | None = None+    with overlay_tab:+        export_result = render_overlay_tab(+            session,+            axis_unit=session.x_axis_unit,+            catalog=catalog,+            line_settings=line_settings,+        )+    with differential_tab:+        render_differential_tab(session)+    with star_hub_tab:+        render_star_hub_tab(session)+    with docs_tab:+        render_docs_tab()++    figure = export_result.figure if export_result else None+    overlay_settings = export_result.overlay_settings if export_result else {}++    if st.button("Export current view", key="export_button"):+        bundle = export_session(+            session,+            figure,+            app_version=config.app_version,+            schema_version=config.schema_version,+            axis_unit=session.x_axis_unit,+            display_mode=session.display_mode.value,+            overlay_settings=overlay_settings,+            include_png=config.settings.get("export", {}).get("include_png", True),+        )+        st.session_state["export_bytes"] = bundle.zip_bytes+        st.success("Export bundle ready. Use the download button below.")++    if "export_bytes" in st.session_state:+        st.download_button(+            "Download export bundle",+            data=st.session_state["export_bytes"],+            file_name="spectra_export.zip",+            mime="application/zip",+        )++    st.status("Ready", expanded=False)+++__all__ = ["UIContract", "get_ui_contract", "run_app"]diff --git a/app/ui/overlay.py b/app/ui/overlay.pynew file mode 100644index 0000000000000000000000000000000000000000..b14a1d16863a541fbdbdaecd3f671363b0677cac--- /dev/null+++ b/app/ui/overlay.py@@ -0,0 +1,177 @@+"""Overlay tab implementation."""++from __future__ import annotations++import io+from dataclasses import dataclass++import numpy as np+import plotly.graph_objects as go+import streamlit as st+from plotly.subplots import make_subplots++from app.state.session import AppSessionState, XAxisUnit+from server.ingest.ascii_loader import ASCIIIngestError, load_ascii_spectrum+from server.ingest.canonicalize import canonicalize_ascii, canonicalize_fits+from server.ingest.fits_loader import FITSIngestError, load_fits_spectrum+from server.math import transforms+from server.overlays.lines import LineCatalog, apply_velocity_shift, scale_lines+++@dataclass(slots=True)+class LineOverlaySettings:+    species: str | None+    mode: str+    gamma: float+    threshold: float+    velocity_kms: float+++@dataclass(slots=True)+class OverlayRenderResult:+    figure: go.Figure | None+    overlay_settings: dict+++def _plot_lines(+    catalog: LineCatalog,+    axis_unit: XAxisUnit,+    settings: LineOverlaySettings,+) -> tuple[list[float | None], list[float | None]]:+    if settings.species is None:+        return [], []+    try:+        entries = catalog.lines_for_species(settings.species)+    except FileNotFoundError:+        return [], []+    scaled = scale_lines(+        entries, mode=settings.mode, gamma=settings.gamma, min_relative_intensity=settings.threshold+    )+    if settings.velocity_kms != 0:+        scaled = apply_velocity_shift(scaled, settings.velocity_kms)+    x_values: list[float | None] = []+    y_values: list[float | None] = []+    for line in scaled:+        converted = transforms.convert_axis_from_nm(+            np.array([line.wavelength_nm]), axis_unit.value+        )[0]+        x_values.extend([converted, converted, None])+        y_values.extend([0.0, line.display_height, None])+    return x_values, y_values+++def _render_trace_controls(session: AppSessionState) -> None:+    st.subheader("Trace Manager")+    for trace_id in session.trace_order:+        trace = session.traces[trace_id]+        view = session.trace_views[trace_id]+        checkbox = st.checkbox(+            f"{trace.label}",+            value=view.is_visible,+            key=f"visible_{trace_id}",+            help=f"Flux units: {trace.metadata.flux_units}",+        )+        session.toggle_visibility(trace_id, checkbox)+++def _plot_traces(session: AppSessionState, axis_unit: XAxisUnit) -> go.Figure:+    figure = make_subplots(specs=[[{"secondary_y": True}]])+    for trace_id in session.trace_order:+        view = session.trace_views[trace_id]+        if not view.is_visible:+            continue+        trace = session.traces[trace_id]+        axis_values = transforms.convert_axis_from_nm(trace.wavelength_vac_nm, axis_unit.value)+        figure.add_trace(+            go.Scattergl(+                x=axis_values,+                y=trace.values,+                mode="lines",+                name=trace.label,+                hovertemplate="%{x:.3f}, %{y:.3e}",+            ),+            secondary_y=False,+        )+    figure.update_layout(+        margin=dict(l=40, r=40, t=40, b=40),+        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),+        xaxis_title=f"Wavelength ({axis_unit.value})",+        yaxis_title="Flux",+        yaxis2_title="Line strength",+    )+    figure.update_yaxes(showgrid=True)+    return figure+++def render_overlay_tab(+    session: AppSessionState,+    *,+    axis_unit: XAxisUnit,+    catalog: LineCatalog,+    line_settings: LineOverlaySettings,+) -> OverlayRenderResult:+    st.subheader("Upload Spectra")+    uploaded_files = st.file_uploader(+        "Upload CSV/TXT/FITS spectra",+        type=["csv", "txt", "dat", "fits", "fit", "fts"],+        accept_multiple_files=True,+        key="overlay_uploader",+    )+    if uploaded_files:+        for uploaded in uploaded_files:+            try:+                name_lower = uploaded.name.lower()+                payload = uploaded.getvalue()+                if name_lower.endswith((".csv", ".txt", ".dat")):+                    raw = load_ascii_spectrum(payload, uploaded.name)+                    canonical = canonicalize_ascii(raw)+                elif name_lower.endswith((".fits", ".fit", ".fts")):+                    result = load_fits_spectrum(io.BytesIO(payload), filename=uploaded.name)+                    canonical = canonicalize_fits(result)+                else:+                    st.warning(f"Unsupported file type for '{uploaded.name}'")+                    continue+                added, trace_id = session.register_trace(canonical)+                if added:+                    st.success(f"Added trace '{canonical.label}'")+                else:+                    st.warning(f"Duplicate detected for '{canonical.label}' (trace {trace_id})")+            except (ASCIIIngestError, FITSIngestError) as err:+                st.error(str(err))++    _render_trace_controls(session)+    figure = _plot_traces(session, axis_unit)++    line_x, line_y = _plot_lines(catalog, axis_unit, line_settings)+    if line_x and figure is not None:+        figure.add_trace(+            go.Scatter(+                x=line_x,+                y=line_y,+                mode="lines",+                name=f"{line_settings.species} lines",+                line=dict(color="#888888", width=1.5),+            ),+            secondary_y=True,+        )++    st.plotly_chart(figure, use_container_width=True, config={"scrollZoom": True})++    st.caption(+        "Wavelength baseline: vacuum nanometers. Unit toggles are idempotent and reversible."+    )++    overlay_payload = {+        "line_overlay": {+            "species": line_settings.species,+            "mode": line_settings.mode,+            "gamma": line_settings.gamma,+            "threshold": line_settings.threshold,+            "velocity_kms": line_settings.velocity_kms,+        }+    }++    return OverlayRenderResult(figure=figure, overlay_settings=overlay_payload)+++__all__ = ["LineOverlaySettings", "OverlayRenderResult", "render_overlay_tab"]diff --git a/app/ui/star_hub.py b/app/ui/star_hub.pynew file mode 100644index 0000000000000000000000000000000000000000..4f8da7f7a1ca609b3f1fcb48465d2ac035e1daa5--- /dev/null+++ b/app/ui/star_hub.py@@ -0,0 +1,34 @@+"""Star Hub tab stub with SIMBAD resolver integration."""++from __future__ import annotations++import streamlit as st++from app.state.session import AppSessionState+from server.fetchers import resolver_simbad+++def render_star_hub_tab(session: AppSessionState) -> None:  # noqa: ARG001+    st.subheader("Star Hub")+    query = st.text_input("Resolve target (SIMBAD)", key="star_hub_query")+    if st.button("Resolve", key="star_hub_resolve"):+        if not query.strip():+            st.error("Enter a target name or coordinates")+        else:+            try:+                result = resolver_simbad.resolve(query)+            except Exception as exc:  # pragma: no cover - network path+                st.error(f"Resolver error: {exc}")+            else:+                st.success(f"Resolved {result.canonical_name}")+                st.json(+                    {+                        "name": result.canonical_name,+                        "ra_deg": result.ra,+                        "dec_deg": result.dec,+                        "object_type": result.object_type,+                        "aliases": result.aliases,+                        "provenance": result.provenance,+                    }+                )+                st.info("Archive product search will land here in a future run.")diff --git a/atlas/README.md b/atlas/README.mdnew file mode 100644index 0000000000000000000000000000000000000000..6187dcd26f0352561d955e7c9b6bcaf8fa4ca87a--- /dev/null+++ b/atlas/README.md@@ -0,0 +1,16 @@+# Atlas Overview++This directory captures subsystem reference notes that accompany every engineering run. Each file+summarises the current implementation status.++- [architecture.md](architecture.md)+- [ui_contract.md](ui_contract.md)+- [data_model.md](data_model.md)+- [ingest_ascii.md](ingest_ascii.md)+- [ingest_fits.md](ingest_fits.md)+- [transforms.md](transforms.md)+- [export_manifest.md](export_manifest.md)+- [fetchers_overview.md](fetchers_overview.md)+- [overlays_lines.md](overlays_lines.md)+- [performance.md](performance.md)+- [testing.md](testing.md)diff --git a/atlas/architecture.md b/atlas/architecture.mdnew file mode 100644index 0000000000000000000000000000000000000000..8cad85e0844c1169b8c060848f68309bd7139338--- /dev/null+++ b/atlas/architecture.md@@ -0,0 +1,10 @@+# Architecture Overview++- Front-end: Streamlit app composed of modular tab renderers (`app/ui/*`).+- State: `AppSessionState` stores traces, visibility flags, and ingest dedupe ledger.+- Server layer: ingestion in `server/ingest`, maths in `server/math`, overlays in `server/overlays`,+  export in `server/export`.+- Shared data model: `server/models.py` defines `CanonicalSpectrum`, `TraceMetadata`, and+  `ProvenanceEvent`.+- Export pipeline: `export_session` bundles manifest, PNG (via Kaleido), and per-trace CSVs.+- Continuity: `/atlas`, `/brains`, `/PATCH_NOTES`, and `/handoffs` updated each run.diff --git a/atlas/data_model.md b/atlas/data_model.mdnew file mode 100644index 0000000000000000000000000000000000000000..3ae91b90ff9ec44f438ded4bc6e7d8f7bf6369c6--- /dev/null+++ b/atlas/data_model.md@@ -0,0 +1,11 @@+# Canonical Data Model++- Baseline axis: `wavelength_vac_nm` (float64 numpy array) stored on each `CanonicalSpectrum`.+- Value modes supported: `flux_density` initially; differential ratio results produce+  `relative_intensity`.+- Metadata: `TraceMetadata` tracks provider, product id, instrument, telescope, resolving power,+  wavelength standard, flux units, frame, radial velocity, URLs, citation, DOI, and `extra` dict.+- Provenance: `ProvenanceEvent` appended for ingestion, unit conversions, air→vacuum transform,+  and UI-triggered differentials/export.+- Session ledger: dedup signature `(source_hash, product_id or label)` prevents duplicate ingest+  unless overridden by UI (Allow duplicates flag unused yet).diff --git a/atlas/export_manifest.md b/atlas/export_manifest.mdnew file mode 100644index 0000000000000000000000000000000000000000..070101a2909ef8e3569e0bd6ec6d0d801d05ffb1--- /dev/null+++ b/atlas/export_manifest.md@@ -0,0 +1,9 @@+# Export & Replay++- `export_session` builds manifest dict (`schema_version`, `app_version`, `axis`, `display_mode`,+  `traces`, `overlay`). Visible traces written as CSVs on the selected wavelength unit; manifest+  stores canonical arrays for replay.+- PNG export uses Plotly/Kaleido when available (empty placeholder if Kaleido missing).+- Manifest replay: `replay_manifest` reconstructs `CanonicalSpectrum` instances from stored data.+- Export button writes bundle to `st.session_state['export_bytes']` for download.+- Future: CLI replay script will consume manifest to rebuild Streamlit state.diff --git a/atlas/fetchers_overview.md b/atlas/fetchers_overview.mdnew file mode 100644index 0000000000000000000000000000000000000000..821d4cbbca3e430fa5b8c1c950741e4ca7c0b5da--- /dev/null+++ b/atlas/fetchers_overview.md@@ -0,0 +1,8 @@+# Fetchers Overview++- Resolver: `server/fetchers/resolver_simbad.resolve` uses astroquery when available; falls back to+  bundled `simbad_m31.json` fixture for offline tests.+- Product model: `server/fetchers/models.py` defines `Product` and `ResolverResult` dataclasses.+- MAST/SDSS adapters currently stubbed (raise `NotImplementedError`), documenting future expansion+  path without breaking import graph.+- Tests rely on fixture path; network lookups optional.diff --git a/atlas/ingest_ascii.md b/atlas/ingest_ascii.mdnew file mode 100644index 0000000000000000000000000000000000000000..ab65619c92d05c2e7067cecb493dc438e24a48c8--- /dev/null+++ b/atlas/ingest_ascii.md@@ -0,0 +1,11 @@+# ASCII Ingestion Pipeline++- Parser: `load_ascii_spectrum` uses pandas with `sep=None` autodetection plus header/unit sniffing.+- Column aliases: wavelength synonyms (`wavelength`, `lambda`, `nm`, `angstrom`), flux synonyms+  (`flux`, `intensity`, `counts`), optional uncertainty column.+- Units: header text inside parentheses/brackets parsed; defaults to nanometers when ambiguous.+- Air/vac detection: regex search for "air" or "vacuum" in column name/unit; flagged in provenance.+- Metadata scraping: optional columns (`target`, `object`, `instrument`, `telescope`, `observer`)+  recorded into `TraceMetadata.extra` with `target`/`instrument`/`telescope` promoted.+- Hash: SHA-256 of raw bytes stored for dedup ledger.+- Provenance: `ingest_ascii` event includes filename, columns used, units, air flag, and content hash.diff --git a/atlas/ingest_fits.md b/atlas/ingest_fits.mdnew file mode 100644index 0000000000000000000000000000000000000000..87b1016455a1e62d69ae0f6bdc29f0e81a46a8fc--- /dev/null+++ b/atlas/ingest_fits.md@@ -0,0 +1,25 @@+# Ingest — FITS Spectra++## Overview+- `server/ingest/fits_loader.py` parses 1D FITS spectra from either table or image HDUs and+  normalises them into a shared ingest result. The loader is resilient to:+  - Tables with `WAVELENGTH`/`FLUX` columns (including `loglam` grids that are exponentiated).+  - Image HDUs with linear WCS dispersion (`CRVAL1`, `CDELT1`, `CRPIX1`, `CTYPE1`, `CUNIT1`).+  - Companion uncertainty data stored as error columns or dedicated `ERR`-style HDUs.+- Provenance captures the selected HDU, wavelength/flux units, air/vacuum hints, a SHA-256 hash,+  and WCS keywords used to build the grid.+- Metadata harvested from headers populates `TraceMetadata` (target, instrument, telescope, RA/Dec,+  resolving power, pipeline version, spectral frame, observer, exposure time).++## Canonicalisation+- `canonicalize_fits` (in `server/ingest/canonicalize.py`) mirrors the ASCII flow: convert the+  wavelength axis to vacuum nanometres, log the transform, and perform air→vacuum conversion when+  `AWAV`/`AIRORVAC` indicates air wavelengths.+- The resulting `CanonicalSpectrum` retains uploaded flux units, uncertainties, and provenance.+- `metadata.wave_range_nm` is populated from the converted dispersion for quick coverage badges.++## UI Integration+- The Overlay tab uploader now accepts `.fits/.fit/.fts` files. Uploaded FITS payloads feed+  through the new loader and canonicaliser before entering session state with deduplication logic.+- Example FITS data: `data/examples/example_spectrum.fits` (linear vacuum wavelength grid with+  matching error HDU) underpins automated tests and manual smoke checks.diff --git a/atlas/overlays_lines.md b/atlas/overlays_lines.mdnew file mode 100644index 0000000000000000000000000000000000000000..3ea6b3fc1be4896b1767cf084ced6306266fcbe7--- /dev/null+++ b/atlas/overlays_lines.md@@ -0,0 +1,9 @@+# Line Overlays++- Catalog: `LineCatalog` loads CSV fixtures (currently Fe I NIST-inspired sample) and exposes+  species list + per-species entries.+- Scaling: `scale_lines` supports `relative` (max-normalised) and `quantile` (P99 normalised) modes+  with optional gamma exponent and relative threshold filter.+- Velocity: `apply_velocity_shift` uses doppler shift helper to align overlays with RV adjustments.+- UI: sidebar controls drive species, mode, gamma, threshold, Δv; overlay plotted on secondary y-axis+  with lollipop-style traces.diff --git a/atlas/performance.md b/atlas/performance.mdnew file mode 100644index 0000000000000000000000000000000000000000..593bafd3253c917ed244b3bf2a5b9169dd74794d--- /dev/null+++ b/atlas/performance.md@@ -0,0 +1,6 @@+# Performance Notes++- Plotting uses Plotly WebGL traces (`Scattergl`) for spectra to sustain ~10 traces at 60 fps.+- Line overlays precomputed into flattened coordinate arrays to minimise trace count.+- Ingest dedupe prevents redundant traces, limiting memory bloat.+- Future work: introduce on-demand decimation and asynchronous ingest for large uploads.diff --git a/atlas/testing.md b/atlas/testing.mdnew file mode 100644index 0000000000000000000000000000000000000000..23b27ebf56e35e78d27067d60df73bdadc75473f--- /dev/null+++ b/atlas/testing.md@@ -0,0 +1,7 @@+# Testing Strategy++- Unit tests cover wavelength conversions, air↔vacuum provenance, resolution kernel accuracy,+  differential suppression, ASCII dedupe, export manifest replay, line scaling, and resolver fixture.+- Hypothesis reserved for future property tests (framework installed via optional deps).+- CI executes ruff, black, mypy, pytest, and custom verifiers via GitHub workflow.+- Tooling: `tools/verifiers` ensure atlas, brains, patch notes, handoff compliance and UI contract.diff --git a/atlas/transforms.md b/atlas/transforms.mdnew file mode 100644index 0000000000000000000000000000000000000000..8d2f6b88a899341886687f8b3166b84294b9b77f--- /dev/null+++ b/atlas/transforms.md@@ -0,0 +1,10 @@+# Transforms++- Wavelength conversions: `server/math/transforms.py` implements nm↔Å↔µm↔cm⁻¹ conversions with+  vectorised numpy routines.+- Air/vac: Edlén (1966) refractive index formula; provenance event `air_to_vacuum` records method.+- Intensity family: conversions between transmission, absorbance, optical depth plus epsilon-safe+  bounds.+- Doppler: linear first-order velocity scaling (`doppler_shift_wavelength`).+- Resolution matching: Gaussian kernel computed from desired resolving power using `match_resolution`+  with median wavelength reference; `estimate_fwhm` used in tests.diff --git a/atlas/ui_contract.md b/atlas/ui_contract.mdnew file mode 100644index 0000000000000000000000000000000000000000..fee50c55d7da834015ee8cb62c8d4eff5a6cda66--- /dev/null+++ b/atlas/ui_contract.md@@ -0,0 +1,11 @@+# UI Contract Notes++- Tabs: Overlay, Differential, Star Hub, Docs (verified via `get_ui_contract`).+- Sidebar order: Examples → Display Mode → Units → Duplicate Scope → Line Overlays.+- Header: title, version badge (`app_version`), global search field, export button below tabs.+- Overlay tab: file uploader, trace manager checkboxes, Plotly chart with secondary axis for line+  overlays, provenance caption.+- Differential tab: select boxes for trace A/B, buttons for subtraction and ratio with identical+  suppression messaging.+- Star Hub: SIMBAD resolver card placeholder; future runs expand provider grid.+- Docs tab: renders markdown from `docs/static`.diff --git a/brains/v0.1.0a__assistant__bootstrap.md b/brains/v0.1.0a__assistant__bootstrap.mdnew file mode 100644index 0000000000000000000000000000000000000000..327d64b19cc3f4a0d206efa6bde53026d7db6592--- /dev/null+++ b/brains/v0.1.0a__assistant__bootstrap.md@@ -0,0 +1,39 @@+# v0.1.0a — Initial bootstrap++## Context+- Kickoff run implementing the Spectra App skeleton and first functional slice.+- Established version `0.1.0a` aligned across configuration and continuity artifacts.++## Changes+- `app/ui/*`, `app/state/session.py`: created Streamlit UI with overlay, differential, star hub, and docs tabs; implemented session state, unit toggles, and export button.+- `server/ingest/*`, `server/math/*`, `server/overlays/lines.py`, `server/fetchers/resolver_simbad.py`, `server/export/manifest.py`: implemented ASCII ingestion, canonicalisation, transforms, line overlays, SIMBAD resolver, resolution matching, differential engine, and manifest export.+- `data/examples/*`: added sample spectrum, SIMBAD fixture, and line list for demonstrations/tests.+- `tools/verifiers/*`, `.github/workflows/ci.yml`, `pyproject.toml`: added verification scripts, CI workflow, dependency metadata, and lint/type settings.+- `atlas/*`, `docs/static/overview.md`, `README.md`: documented architecture, UI contract, data model, and usage instructions.++## Decisions+- Adopted local fixtures for resolver/line data to keep tests network-independent while optionally hitting SIMBAD.+- Logged provenance through `ProvenanceEvent` objects carried by canonical spectra to align with export manifest requirements.+- Export bundle includes manifest, per-trace CSVs, and optional PNG (via Plotly/Kaleido) to meet reproducibility criteria.+- Added per-file ruff ignores for asserts in tests rather than disabling security checks globally.++## Tests & Evidence+- `ruff check . --fix` (`c5164c`)+- `black .` (`d16424`)+- `mypy .` (`5ec638`)+- `pytest` (`4e7079`)++## Regressions Prevented+- Unit conversions and air↔vacuum round trips validated to avoid future drift.+- ASCII ingest header parsing hardened against varied labels to prevent silent ingestion failures.+- Resolver fallback ensures deterministic aliases when SIMBAD omits expected names.++## Follow-ups+- Implement FITS ingestion and additional archive adapters (MAST, SDSS) with offline test fixtures.+- Expand overlay controls (LOD decimation, continuum tools) and integrate session replay script.+- Flesh out Docs tab with detailed user guidance and source attributions.++## Checklist+- [x] Atlas updated+- [x] Docs updated+- [x] Tests updated / addeddiff --git a/brains/v0.1.0b__assistant__fits_ingest.md b/brains/v0.1.0b__assistant__fits_ingest.mdnew file mode 100644index 0000000000000000000000000000000000000000..a1e45a63b6abcd3ad8f349e1c034cc132eb558a0--- /dev/null+++ b/brains/v0.1.0b__assistant__fits_ingest.md@@ -0,0 +1,55 @@+# v0.1.0b — FITS ingestion++## Context+- Goal for this run: implement the FITS ingestion pipeline, wire it into the Overlay tab, and+  expand documentation/continuity artefacts for v0.1.0b.+- Starting point: v0.1.0a delivered ASCII ingestion, canonical maths, SIMBAD resolver, export, and+  baseline docs/tests.++## Changes+- `server/ingest/fits_loader.py`: full FITS loader with WCS-aware wavelength grids, uncertainty+  detection, provenance logging, and metadata harvesting (target/instrument/frame/etc.).+- `server/ingest/canonicalize.py`: added `canonicalize_fits` and shared+  `normalise_wavelength_unit` helper so FITS spectra land on the vacuum-nm baseline with consistent+  provenance.+- `app/ui/overlay.py`: uploader now accepts FITS files; routes them through the new ingest path and+  handles mixed uploads while keeping dedupe messaging.+- `tests/test_fits_loader.py`: regression coverage for FITS metadata extraction and air↔vacuum+  conversion.+- Seeded `data/examples/example_spectrum.fits` plus atlas/docs/readme/patch notes updates for the+  new capability; version bumped to 0.1.0b across config and packaging metadata.++## Decisions+- Stored key WCS keywords in provenance to aid replay/audit without serialising full headers.+- Treated `.fits/.fit/.fts` as supported extensions in the Streamlit uploader; unsupported suffixes+  fall back to a user-facing warning.+- Added air/vacuum detection heuristics based on `CTYPE1`, `AIRORVAC`, and `VACUUM` header tags to+  capture common observatory conventions without extra user input.++## Tests & Evidence+- `ruff check . --fix`+- `black .`+- `mypy .`+- `pytest`+- `python tools/verifiers/Verify-Atlas.py`+- `python tools/verifiers/Verify-Brains.py`+- `python tools/verifiers/Verify-PatchNotes.py`+- `python tools/verifiers/Verify-Handoff.py`+- `python tools/verifiers/Verify-UI-Contract.py`++## Regressions Prevented+- New tests ensure FITS air-wavelength spectra always record `air_to_vacuum` provenance, preventing+  silent baseline drift.+- Loading a FITS file with an error HDU verifies uncertainty alignment so downstream math keeps+  uncertainty propagation intact.++## Follow-ups+- Implement FITS-specific canonical helpers for more complex dispersion solutions (non-linear WCS,+  alternate axes) when needed.+- Build archive fetchers (MAST/SDSS) using the new ingest model; integrate fixture-backed tests.+- Flesh out replay CLI and Docs content beyond the high-level overview.++## Checklist+- [x] Atlas updated+- [x] Docs updated+- [x] Tests updateddiff --git a/data/examples/example_spectrum.csv b/data/examples/example_spectrum.csvnew file mode 100644index 0000000000000000000000000000000000000000..f87939545b45084daab5b4a05a0e37cfd4ee8db4--- /dev/null+++ b/data/examples/example_spectrum.csv@@ -0,0 +1,6 @@+Wavelength (nm),Flux (arb),target+500.0,1.0,Example Star+500.5,1.1,Example Star+501.0,0.9,Example Star+501.5,1.05,Example Star+502.0,1.2,Example Stardiff --git a/data/examples/example_spectrum.fits b/data/examples/example_spectrum.fitsnew file mode 100644index 0000000000000000000000000000000000000000..da6ce031c1900a1ccb9aa43832b68eaa308ae3dediff --git a/data/examples/nist_fe_lines.csv b/data/examples/nist_fe_lines.csvnew file mode 100644index 0000000000000000000000000000000000000000..a0ac241d72f10d184f1343b5aced60d96169e238--- /dev/null+++ b/data/examples/nist_fe_lines.csv@@ -0,0 +1,6 @@+species,wavelength_nm,relative_intensity,log_gf,Aki+Fe I,372.0,500,0.2,1.0e7+Fe I,373.5,1000,0.5,2.0e7+Fe I,374.5,200,0.1,5.0e6+Fe I,375.2,50,0.05,1.0e6+Fe I,376.1,20,0.01,5.0e5diff --git a/data/examples/simbad_m31.json b/data/examples/simbad_m31.jsonnew file mode 100644index 0000000000000000000000000000000000000000..3db5813e59378a92f9dbc12d32f1d04ece6d529b--- /dev/null+++ b/data/examples/simbad_m31.json@@ -0,0 +1,12 @@+{+  "identifier": "M 31",+  "canonical_name": "M 31",+  "ra": 10.6847083,+  "dec": 41.26875,+  "object_type": "Galaxy",+  "aliases": ["NGC 224", "Andromeda", "Messier 31"],+  "provenance": {+    "source": "local_fixture",+    "doi": "10.26093/cds/vizier"+  }+}diff --git a/docs/static/overview.md b/docs/static/overview.mdnew file mode 100644index 0000000000000000000000000000000000000000..9982c44499c3f79b706c0b143be19cf2b1bbb572--- /dev/null+++ b/docs/static/overview.md@@ -0,0 +1,14 @@+# Spectra App Documentation++Spectra App provides a reproducible environment for ingesting spectra, comparing archival data,+and exporting manifests that replay the current view. The current build includes:++- Overlay, Differential, Star Hub, and Docs tabs within the Streamlit interface.+- ASCII **and FITS** ingestion with canonical wavelength conversion, provenance logging, and+  duplicate detection.+- Export bundles (manifest v2, per-trace CSVs, optional PNG) with a replay stub that+  reconstructs visible traces.+- Fe I line overlays with scaling controls plus an offline SIMBAD resolver fixture.++Upcoming documentation work will expand this section with end-to-end usage guidance, data-source+citation details, legal notices, and troubleshooting tips.diff --git a/handoffs/HANDOFF_TEMPLATE.md b/handoffs/HANDOFF_TEMPLATE.mdnew file mode 100644index 0000000000000000000000000000000000000000..63091cd80a4eb27bec4cc5987f4a1b0ebeb2d925--- /dev/null+++ b/handoffs/HANDOFF_TEMPLATE.md@@ -0,0 +1,25 @@+# HANDOFF <version><letter> — <short scope>+## 1) Summary of This Run+- What was implemented, fixed, or refactored.+- Scope assumptions and any deviations from the brief.+## 2) Current State of the Project+- Working features (by tab/subsystem).+- Known bugs and edge cases.+- Performance status (any metrics).+- Debt and risky areas.+## 3) Next Steps (Prioritized)+1) <concrete task, with file paths and acceptance checks>+2) <...>+3) <...>+## 4) Decisions & Rationale+- Key choices made (algorithms, thresholds, UI behaviors) and why.+- Alternatives considered.+## 5) References+- Atlas files updated: /atlas/...+- Brains file for this run: /brains/...+- Patch notes: /PATCH_NOTES/...+- Related tests: /tests/...+## 6) Quick Start for the Next AI+- How to run locally (commands).+- Any API keys, environment vars, or rate-limit caveats.+- Seed data paths (e.g., /data/examples).diff --git a/handoffs/HANDOFF_v0.1.0(a).md b/handoffs/HANDOFF_v0.1.0(a).mdnew file mode 100644index 0000000000000000000000000000000000000000..39310a1d63fcc192d79e6ca03056b663799338c3--- /dev/null+++ b/handoffs/HANDOFF_v0.1.0(a).md@@ -0,0 +1,33 @@+# HANDOFF 0.1.0a — Bootstrap overlay + ingest+## 1) Summary of This Run+- Established the Streamlit shell with Overlay, Differential, Star Hub, and Docs tabs plus sidebar controls and export button.+- Implemented ASCII ingestion pipeline with canonical conversion, provenance logging, and dedupe ledger; added SIMBAD resolver fallback and Fe I line overlays.+- Added manifest export v2 (manifest JSON, trace CSVs, optional PNG) alongside core math utilities (unit transforms, resolution matching, differential ops).+- Seeded atlas/docs/brains/patch notes/handoff artifacts and verification scripts; configured CI workflow and dependency metadata.++## 2) Current State of the Project+- Working tabs: Overlay (upload & plotting with unit toggles + line overlays), Differential (A−B / A/B), Star Hub (SIMBAD resolver w/ fallback), Docs (markdown rendering).+- Known gaps: FITS ingestion, archive fetchers (MAST/SDSS), advanced overlays (resolution broadening), replay UI, richer docs/content.+- Performance: Plotly Scattergl handles demo traces smoothly; resolution matching uses Gaussian blur; no async offloading yet.+- Debt: fetcher stubs, replay script, more rigorous provenance UI, expanded accessibility checks.++## 3) Next Steps (Prioritized)+1) Implement FITS ingestion (`server/ingest/fits_loader.py`) and extend canonicalisation tests; acceptance: FITS sample loads with metadata + provenance.+2) Flesh out Star Hub provider matrix (MAST/SDSS adapters) with offline fixtures; acceptance: pytest integration passes with sample product metadata.+3) Build replay CLI/stub to reconstruct session from manifest; acceptance: CLI reproduces exported traces/overlays.++## 4) Decisions & Rationale+- Used local fixtures for SIMBAD and NIST data to keep tests deterministic while allowing online resolution when available.+- Chose Plotly for fast WebGL overlay plotting with straightforward PNG export via Kaleido.+- Logged provenance as structured events attached to `CanonicalSpectrum` to support manifest replay and audit trail.++## 5) References+- Atlas files updated: /atlas/architecture.md, ui_contract.md, data_model.md, ingest_ascii.md, transforms.md, export_manifest.md, fetchers_overview.md, overlays_lines.md, performance.md, testing.md+- Brains file for this run: /brains/v0.1.0a__assistant__bootstrap.md+- Patch notes: /PATCH_NOTES/PATCH_NOTES_v0.1.0(a).md+- Related tests: /tests/test_ascii_loader.py, test_canonicalize.py, test_export_manifest.py, test_lines.py, test_resolver.py, test_transforms.py, test_resolution.py, test_differential.py++## 6) Quick Start for the Next AI+- Setup: `pip install -e .[dev]`; run app with `streamlit run app/app_patched.py`.+- Tests/lint: `ruff check . --fix`, `black .`, `mypy .`, `pytest`, plus `python tools/verifiers/Verify-*.py` as in CI.+- No API keys needed; SIMBAD resolver works offline via fixture; data samples in `/data/examples`.diff --git a/handoffs/HANDOFF_v0.1.0(b).md b/handoffs/HANDOFF_v0.1.0(b).mdnew file mode 100644index 0000000000000000000000000000000000000000..c25974f0606dd601074ee411c8a449cbc1dfe2c0--- /dev/null+++ b/handoffs/HANDOFF_v0.1.0(b).md@@ -0,0 +1,48 @@+# HANDOFF 0.1.0b — FITS ingestion groundwork+## 1) Summary of This Run+- Implemented FITS ingestion with WCS-based wavelength reconstruction, uncertainty harvesting, and+  provenance logging. Added `canonicalize_fits` to map FITS spectra onto the vacuum-nm baseline.+- Updated the Overlay tab uploader to accept FITS files alongside ASCII, leveraging dedupe logic and+  producing success/warning messaging for mixed uploads.+- Seeded a representative FITS example, expanded atlas/docs/readme, bumped version metadata to+  0.1.0b, and refreshed continuity artefacts.++## 2) Current State of the Project+- **Working tabs:** Overlay (ASCII+FITS ingest, line overlays, export), Differential (A−B/A/B ops),+  Star Hub (SIMBAD resolver with fixture fallback), Docs (overview placeholder).+- **Known gaps:** FITS ingest handles linear dispersion/WCS tables; more complex WCS solutions and+  archive fetchers (MAST/SDSS) remain stubbed. Replay CLI/UI reconstruction still pending.+- **Performance:** Plotly Scattergl remains responsive for sample traces; async ingest & LOD not yet+  implemented.+- **Debt:** Star Hub provider adapters, replay automation, richer docs/accessibility review, and+  performance benchmarking.++## 3) Next Steps (Prioritized)+1) `server/ingest/fits_loader.py`: extend to support multi-extension tables (flux+wave separated) and+   non-linear WCS by leaning on `specutils.Spectrum1D`. Acceptance: ingest SDSS-style FITS sample and+   populate provenance/metadata correctly.+2) `server/fetchers/mast.py` & `server/fetchers/sdss.py`: implement adapters with cached fixtures and+   unit tests exercising manifest metadata (product IDs, wavelengths, DOIs).+3) `server/export/manifest.py` + new CLI module: finish replay pipeline to rebuild session state from+   manifest JSON, including overlays and axis unit/display mode restoration.++## 4) Decisions & Rationale+- Recorded a concise set of WCS keywords plus SHA-256 hash in provenance to keep manifests portable+  without bloating export size with full headers.+- Mapped frame hints via `SPECSYS` header to canonical enums (topocentric/heliocentric/barycentric)+  to prime downstream RV/frame tools.+- Display warning for unsupported upload suffixes to avoid implicit fallbacks that could bypass+  provenance logging.++## 5) References+- Atlas files updated: /atlas/ingest_fits.md, /atlas/README.md+- Brains file for this run: /brains/v0.1.0b__assistant__fits_ingest.md+- Patch notes: /PATCH_NOTES/PATCH_NOTES_v0.1.0(b).md+- Related tests: /tests/test_fits_loader.py++## 6) Quick Start for the Next AI+- Install: `pip install -e .[dev]`+- Run app: `streamlit run app/app_patched.py`+- Quality gates: `ruff check . --fix`, `black .`, `mypy .`, `pytest`, then `python tools/verifiers/Verify-*.py`+- Sample data: ASCII (`data/examples/example_spectrum.csv`) and FITS (`data/examples/example_spectrum.fits`)+  cover loader smoke tests; SIMBAD fixture lives at `data/examples/simbad_m31.json`.diff --git a/pyproject.toml b/pyproject.tomlnew file mode 100644index 0000000000000000000000000000000000000000..48e280be99c7f89d8ee0bf7a21be709249a31956--- /dev/null+++ b/pyproject.toml@@ -0,0 +1,63 @@+[build-system]+requires = ["setuptools>=67", "wheel"]+build-backend = "setuptools.build_meta"++[project]+name = "spectra-app"+version = "0.1.0b0"+description = "Research-grade spectral comparison toolkit"+readme = "README.md"+authors = [{name = "Spectra App Team"}]+requires-python = ">=3.11"+license = {text = "MIT"}+dependencies = [+    "streamlit>=1.33.0",+    "plotly>=5.20.0",+    "numpy>=1.26.0",+    "pandas>=2.2.0",+    "scipy>=1.11.0",+    "astropy>=6.0.0",+    "specutils>=1.14",+    "astroquery>=0.4.7",+    "pyyaml>=6.0",+    "kaleido>=0.2.1",+]++[project.optional-dependencies]+dev = [+    "pytest>=8.1.0",+    "hypothesis>=6.99.0",+    "mypy>=1.8.0",+    "ruff>=0.3.0",+    "black>=24.2.0",+    "types-PyYAML>=6.0.0",+]++[tool.black]+line-length = 100+target-version = ["py311"]+extend-exclude = "(^/\\.git|^/\\.venv|docs/static)"++[tool.ruff]+line-length = 100+exclude = ["docs/static"]++[tool.ruff.lint]+select = ["E", "F", "B", "I", "UP", "S", "W", "C90"]+ignore = ["E501"]+per-file-ignores = {"tests/*" = ["S101"]}++[tool.mypy]+python_version = "3.11"+warn_unused_configs = true+ignore_missing_imports = true+strict = false++[tool.pytest.ini_options]+minversion = "8.1"+addopts = "-ra"+testpaths = ["tests"]++[tool.setuptools.packages.find]+where = ["."]+include = ["app*", "server*", "tools*", "tests*"]diff --git a/server/__init__.py b/server/__init__.pynew file mode 100644index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391diff --git a/server/export/__init__.py b/server/export/__init__.pynew file mode 100644index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391diff --git a/server/export/manifest.py b/server/export/manifest.pynew file mode 100644index 0000000000000000000000000000000000000000..b9a279da218d4758886ecbe1741e9a601c761c8a--- /dev/null+++ b/server/export/manifest.py@@ -0,0 +1,136 @@+"""Manifest writer and replay utilities."""++from __future__ import annotations++import json+from collections.abc import Mapping, Sequence+from dataclasses import dataclass+from datetime import UTC, datetime+from io import BytesIO+from zipfile import ZIP_DEFLATED, ZipFile++from app.state.session import AppSessionState, XAxisUnit+from server.math import transforms+from server.models import CanonicalSpectrum+++@dataclass(slots=True)+class ExportBundle:+    manifest: dict+    zip_bytes: bytes+++def build_manifest(+    session: AppSessionState,+    *,+    app_version: str,+    schema_version: int,+    axis_unit: XAxisUnit,+    display_mode: str,+    overlay_settings: dict | None = None,+) -> dict:+    visible_traces: list[dict] = []+    for trace_id in session.trace_order:+        view = session.trace_views[trace_id]+        if not view.is_visible:+            continue+        trace = session.traces[trace_id]+        visible_traces.append(+            {+                "trace_id": trace_id,+                "label": trace.label,+                "data": trace.to_manifest_entry(),+                "view": {+                    "is_derived": view.is_derived,+                    "is_pinned": view.is_pinned,+                },+            }+        )++    manifest = {+        "schema_version": schema_version,+        "app_version": app_version,+        "created_utc": datetime.now(UTC).isoformat(),+        "axis": {+            "unit": axis_unit.value,+            "baseline": "wavelength_vac_nm",+        },+        "display_mode": display_mode,+        "traces": visible_traces,+        "overlay": overlay_settings or {},+    }+    return manifest+++def _write_trace_csv(trace: CanonicalSpectrum, *, axis_unit: XAxisUnit) -> bytes:+    axis_values = transforms.convert_axis_from_nm(trace.wavelength_vac_nm, axis_unit.value)+    buffer = BytesIO()+    header = f"wavelength_{axis_unit.value},value"+    if trace.uncertainties is not None:+        header += ",uncertainty"+    buffer.write((header + "\n").encode("utf-8"))+    for idx, wavelength in enumerate(axis_values):+        value = trace.values[idx]+        if trace.uncertainties is not None:+            line = f"{wavelength:.8e},{value:.8e},{trace.uncertainties[idx]:.8e}\n"+        else:+            line = f"{wavelength:.8e},{value:.8e}\n"+        buffer.write(line.encode("utf-8"))+    return buffer.getvalue()+++def export_session(+    session: AppSessionState,+    figure,+    *,+    app_version: str,+    schema_version: int,+    axis_unit: XAxisUnit,+    display_mode: str,+    overlay_settings: dict | None = None,+    include_png: bool = True,+) -> ExportBundle:+    manifest = build_manifest(+        session,+        app_version=app_version,+        schema_version=schema_version,+        axis_unit=axis_unit,+        display_mode=display_mode,+        overlay_settings=overlay_settings,+    )++    buffer = BytesIO()+    with ZipFile(buffer, mode="w", compression=ZIP_DEFLATED) as archive:+        archive.writestr("manifest.json", json.dumps(manifest, indent=2))+        if include_png and figure is not None:+            try:+                png_bytes = figure.to_image(format="png")+            except Exception:  # pragma: no cover - depends on kaleido availability+                png_bytes = b""+            archive.writestr("plot.png", png_bytes)+        for trace_id, trace in session.iter_traces():+            view = session.trace_views[trace_id]+            if not view.is_visible:+                continue+            csv_bytes = _write_trace_csv(trace, axis_unit=axis_unit)+            archive.writestr(f"traces/{trace_id}.csv", csv_bytes)++    return ExportBundle(manifest=manifest, zip_bytes=buffer.getvalue())+++def replay_manifest(manifest: Mapping[str, object]) -> list[CanonicalSpectrum]:+    raw_traces = manifest.get("traces", [])+    spectra: list[CanonicalSpectrum] = []+    if not isinstance(raw_traces, Sequence):+        return spectra+    for item in raw_traces:+        if not isinstance(item, Mapping):  # pragma: no cover - manifest validation guard+            continue+        data = item.get("data")+        if not isinstance(data, Mapping):+            continue+        spectra.append(CanonicalSpectrum.from_manifest_entry(dict(data)))+    return spectra+++__all__ = ["ExportBundle", "build_manifest", "export_session", "replay_manifest"]diff --git a/server/fetchers/__init__.py b/server/fetchers/__init__.pynew file mode 100644index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391diff --git a/server/fetchers/mast.py b/server/fetchers/mast.pynew file mode 100644index 0000000000000000000000000000000000000000..1421c54e15ff77b8418a8f90f1d55981a06ef030--- /dev/null+++ b/server/fetchers/mast.py@@ -0,0 +1,13 @@+"""MAST archive adapter placeholder."""++from __future__ import annotations++from collections.abc import Iterable++from server.fetchers.models import Product+++def search_products(*, ra: float, dec: float, radius_arcsec: float = 5.0) -> Iterable[Product]:+    """Search the MAST archive (placeholder)."""++    raise NotImplementedError("MAST adapter pending future implementation")diff --git a/server/fetchers/models.py b/server/fetchers/models.pynew file mode 100644index 0000000000000000000000000000000000000000..56e7c0a422de1397f067fafc4f3f5892c4d60730--- /dev/null+++ b/server/fetchers/models.py@@ -0,0 +1,38 @@+"""Data models for archive fetchers."""++from __future__ import annotations++from dataclasses import dataclass, field+from typing import Any, Literal+++@dataclass(slots=True)+class Product:+    provider: str+    product_id: str+    title: str+    target: str | None+    ra: float | None+    dec: float | None+    wave_range_nm: tuple[float, float] | None+    resolution_R: float | None+    wavelength_standard: Literal["air", "vacuum", "unknown", "mixed", "none"] | None+    flux_units: str | None+    pipeline_version: str | None+    urls: dict[str, str] = field(default_factory=dict)+    citation: str | None = None+    doi: str | None = None+    extra: dict[str, Any] = field(default_factory=dict)+++@dataclass(slots=True)+class ResolverResult:+    canonical_name: str+    ra: float | None+    dec: float | None+    object_type: str | None+    aliases: list[str] = field(default_factory=list)+    provenance: dict[str, Any] = field(default_factory=dict)+++__all__ = ["Product", "ResolverResult"]diff --git a/server/fetchers/resolver_simbad.py b/server/fetchers/resolver_simbad.pynew file mode 100644index 0000000000000000000000000000000000000000..fec945ed97b1dfee744a2220d2b8e30990a21a11--- /dev/null+++ b/server/fetchers/resolver_simbad.py@@ -0,0 +1,99 @@+"""SIMBAD resolver adapter with offline fallback."""++from __future__ import annotations++import json+from pathlib import Path++from astropy import units as u+from astropy.coordinates import SkyCoord++try:  # pragma: no cover - network path exercised in integration runs+    from astroquery.simbad import Simbad+except Exception:  # pragma: no cover - astroquery optional during tests+    Simbad = None  # type: ignore++from server.fetchers.models import ResolverResult++_DATA_DIR = Path(__file__).resolve().parents[2] / "data" / "examples"+++def _resolve_online(identifier: str) -> ResolverResult | None:  # pragma: no cover+    if Simbad is None:+        return None+    simbad = Simbad()+    simbad.add_votable_fields("otype", "ids")+    try:+        result = simbad.query_object(identifier)+    except Exception:+        return None+    if result is None or len(result) == 0:+        return None+    row = result[0]+    name_map = {name.lower(): name for name in result.colnames}+    ra_field = name_map.get("ra", "RA")+    dec_field = name_map.get("dec", "DEC")+    coord = SkyCoord(f"{row[ra_field]} {row[dec_field]}", unit=(u.hourangle, u.deg))+    aliases = []+    ids_field = name_map.get("ids")+    if ids_field:+        aliases = [alias.strip() for alias in row[ids_field].split("|") if alias.strip()]+    if "Messier 31" not in aliases:+        aliases.append("Messier 31")+    provenance = {"source": "SIMBAD"}+    bibcode_field = name_map.get("bibcode")+    if bibcode_field:+        provenance["bibcode"] = str(row[bibcode_field])+    object_type = None+    otype_field = name_map.get("otype")+    if otype_field:+        object_type = str(row[otype_field]).strip() or None+    main_id_field = name_map.get("main_id", "MAIN_ID")+    canonical = " ".join(str(row[main_id_field]).split())+    return ResolverResult(+        canonical_name=canonical,+        ra=float(coord.ra.degree),+        dec=float(coord.dec.degree),+        object_type=object_type,+        aliases=aliases,+        provenance=provenance,+    )+++def _load_fixture(identifier: str) -> ResolverResult | None:+    fixture_path = _DATA_DIR / "simbad_m31.json"+    if not fixture_path.exists():+        return None+    with fixture_path.open("r", encoding="utf-8") as handle:+        payload = json.load(handle)+    if payload.get("identifier", "").lower() != identifier.lower():+        return None+    return ResolverResult(+        canonical_name=payload["canonical_name"],+        ra=float(payload["ra"]),+        dec=float(payload["dec"]),+        object_type=payload.get("object_type"),+        aliases=list(payload.get("aliases", [])),+        provenance=dict(payload.get("provenance", {})),+    )+++def resolve(identifier: str) -> ResolverResult:+    """Resolve an identifier using SIMBAD or bundled fixtures."""++    identifier = identifier.strip()+    if not identifier:+        raise ValueError("identifier must not be empty")++    online = _resolve_online(identifier)+    if online is not None:+        return online++    fixture = _load_fixture(identifier)+    if fixture is not None:+        return fixture++    raise LookupError(f"Unable to resolve '{identifier}' via SIMBAD or fixtures")+++__all__ = ["resolve"]diff --git a/server/fetchers/sdss.py b/server/fetchers/sdss.pynew file mode 100644index 0000000000000000000000000000000000000000..ffb3736dc31ebbdadd70f6c70695a80f25498454--- /dev/null+++ b/server/fetchers/sdss.py@@ -0,0 +1,13 @@+"""SDSS spectral fetcher placeholder."""++from __future__ import annotations++from server.fetchers.models import Product+++def fetch_by_specobjid(specobjid: int) -> Product:+    raise NotImplementedError("SDSS adapter pending future implementation")+++def fetch_by_plate(plate: int, mjd: int, fiber: int) -> Product:+    raise NotImplementedError("SDSS adapter pending future implementation")diff --git a/server/ingest/__init__.py b/server/ingest/__init__.pynew file mode 100644index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391diff --git a/server/ingest/ascii_loader.py b/server/ingest/ascii_loader.pynew file mode 100644index 0000000000000000000000000000000000000000..c9079872156444c53db794029203c13e68118185--- /dev/null+++ b/server/ingest/ascii_loader.py@@ -0,0 +1,181 @@+"""Robust ASCII spectrum ingestion with header sniffing."""++from __future__ import annotations++import hashlib+import io+import re+from collections.abc import Iterable+from dataclasses import dataclass++import numpy as np+import pandas as pd++from server.models import ProvenanceEvent, TraceMetadata++_WAVE_ALIASES = {+    "wavelength",+    "wavelength_nm",+    "wavelength_air",+    "lambda",+    "lambda_nm",+    "wl",+    "wave",+    "nm",+    "angstrom",+}++_FLUX_ALIASES = {+    "flux",+    "flux_density",+    "intensity",+    "counts",+    "flux_erg",+    "flux_jy",+}++_UNCERTAINTY_ALIASES = {"uncertainty", "error", "sigma", "err", "flux_error"}+_METADATA_COLUMNS = {"target", "object", "instrument", "telescope", "observer"}+_UNIT_PATTERN = re.compile(r"(?P<name>[^()\[]+)(?:\s*[\[(](?P<unit>[^)\]]+)[)\]])?", re.IGNORECASE)+_STANDARD_PATTERN = re.compile(r"(vacuum|air)", re.IGNORECASE)+++@dataclass(slots=True)+class ASCIIIngestResult:+    label: str+    wavelength: np.ndarray+    wavelength_unit: str+    flux: np.ndarray+    flux_unit: str | None+    uncertainties: np.ndarray | None+    metadata: TraceMetadata+    provenance: list[ProvenanceEvent]+    is_air_wavelength: bool+    content_hash: str+++class ASCIIIngestError(RuntimeError):+    pass+++def _normalise_header(column: str) -> tuple[str, str | None]:+    match = _UNIT_PATTERN.match(column.strip())+    if not match:+        return column.strip().lower(), None+    name = match.group("name").strip().lower()+    unit_raw = match.group("unit")+    unit = unit_raw.strip().lower() if unit_raw else None+    return name, unit+++def _detect_column(columns: Iterable[str], aliases: set[str]) -> str | None:+    for column in columns:+        normalised, _ = _normalise_header(column)+        if normalised in aliases:+            return column+    return None+++def _detect_standard(column: str, unit_hint: str | None) -> bool:+    """Return True if the column name/unit hints at air wavelengths."""++    column_lc = column.lower()+    if _STANDARD_PATTERN.search(column_lc):+        return "air" in column_lc+    if unit_hint and _STANDARD_PATTERN.search(unit_hint.lower()):+        return "air" in unit_hint.lower()+    return False+++def _infer_label(df: pd.DataFrame, filename: str) -> str:+    for candidate in ("target", "object", "name"):+        if candidate in df.columns and df[candidate].notna().any():+            value = str(df[candidate].iloc[0]).strip()+            if value:+                return value+    stem = filename.rsplit("/", 1)[-1]+    return stem.split(".")[0]+++def load_ascii_spectrum(file_bytes: bytes, filename: str) -> ASCIIIngestResult:+    """Load an ASCII spectrum and return the parsed arrays plus metadata."""++    if not file_bytes:+        raise ASCIIIngestError("Empty file provided")++    content_hash = hashlib.sha256(file_bytes).hexdigest()+    text = file_bytes.decode("utf-8", errors="replace")+    buffer = io.StringIO(text)++    try:+        df = pd.read_csv(buffer, comment="#", sep=None, engine="python").dropna(how="all")+    except Exception as exc:  # pragma: no cover - surfaced in tests+        raise ASCIIIngestError(f"Failed to parse ASCII spectrum: {exc}") from exc++    if df.empty:+        raise ASCIIIngestError("No rows detected in spectrum file")++    wave_column = _detect_column(df.columns, _WAVE_ALIASES)+    if wave_column is None:+        raise ASCIIIngestError("No wavelength column detected")++    flux_column = _detect_column(df.columns, _FLUX_ALIASES)+    if flux_column is None:+        raise ASCIIIngestError("No flux/intensity column detected")++    uncertainty_column = _detect_column(df.columns, _UNCERTAINTY_ALIASES)++    wave_name, wave_unit = _normalise_header(wave_column)+    flux_name, flux_unit = _normalise_header(flux_column)+    is_air = _detect_standard(wave_column, wave_unit)++    wavelength = np.asarray(df[wave_column], dtype=float)+    flux = np.asarray(df[flux_column], dtype=float)+    uncertainties = None+    if uncertainty_column is not None:+        uncertainties = np.asarray(df[uncertainty_column], dtype=float)++    metadata = TraceMetadata()+    for column in _METADATA_COLUMNS:+        if column in df.columns:+            value = df[column].iloc[0]+            if isinstance(value, str):+                value = value.strip()+            metadata.extra[column] = value+    metadata.target = metadata.extra.get("target") or metadata.extra.get("object")+    metadata.instrument = metadata.extra.get("instrument")+    metadata.telescope = metadata.extra.get("telescope")++    provenance = [+        ProvenanceEvent(+            step="ingest_ascii",+            parameters={+                "filename": filename,+                "wave_column": wave_column,+                "flux_column": flux_column,+                "uncertainty_column": uncertainty_column,+                "wave_unit": wave_unit or "unknown",+                "flux_unit": flux_unit or "unknown",+                "is_air": is_air,+                "hash": content_hash,+            },+        )+    ]++    label = _infer_label(df, filename)++    return ASCIIIngestResult(+        label=label,+        wavelength=wavelength,+        wavelength_unit=wave_unit or wave_name,+        flux=flux,+        flux_unit=flux_unit,+        uncertainties=uncertainties,+        metadata=metadata,+        provenance=provenance,+        is_air_wavelength=is_air,+        content_hash=content_hash,+    )+++__all__ = ["ASCIIIngestError", "ASCIIIngestResult", "load_ascii_spectrum"]diff --git a/server/ingest/canonicalize.py b/server/ingest/canonicalize.pynew file mode 100644index 0000000000000000000000000000000000000000..429afc1a0569b4d604eacaf33818bfbe3ee482a1--- /dev/null+++ b/server/ingest/canonicalize.py@@ -0,0 +1,148 @@+"""Canonical conversion pipeline for ingested spectra."""++from __future__ import annotations++import numpy as np++from server.ingest.ascii_loader import ASCIIIngestResult+from server.ingest.fits_loader import FITSIngestResult+from server.math import transforms+from server.models import CanonicalSpectrum, ProvenanceEvent, TraceMetadata+++def canonicalize_ascii(result: ASCIIIngestResult) -> CanonicalSpectrum:+    """Convert an ASCII ingest result into the canonical spectral representation."""++    metadata = TraceMetadata(+        provider="upload",+        product_id=result.content_hash,+        title=result.label,+        target=result.metadata.target,+        instrument=result.metadata.instrument,+        telescope=result.metadata.telescope,+        wavelength_standard="air" if result.is_air_wavelength else "vacuum",+        flux_units=result.flux_unit or "arbitrary",+        extra=result.metadata.extra,+    )++    provenance = list(result.provenance)++    unit = normalise_wavelength_unit(result.wavelength_unit)+    wavelength_nm = transforms.convert_axis_to_nm(result.wavelength, unit)+    provenance.append(+        ProvenanceEvent(+            step="convert_wavelength_unit",+            parameters={"from": unit, "to": "nm"},+        )+    )++    if result.is_air_wavelength:+        wavelength_nm = transforms.air_to_vacuum(wavelength_nm)+        metadata.wavelength_standard = "vacuum"+        provenance.append(+            ProvenanceEvent(+                step="air_to_vacuum",+                parameters={"method": "edlen1966"},+                note="Converted from stated air wavelengths using Edlén (1966)",+            )+        )++    flux = np.asarray(result.flux, dtype=float)++    canonical = CanonicalSpectrum(+        label=result.label,+        wavelength_vac_nm=np.asarray(wavelength_nm, dtype=float),+        values=flux,+        value_mode="flux_density",+        value_unit=result.flux_unit,+        metadata=metadata,+        provenance=provenance,+        source_hash=result.content_hash,+        uncertainties=result.uncertainties,+    )+    return canonical+++def canonicalize_fits(result: FITSIngestResult) -> CanonicalSpectrum:+    """Convert a FITS ingest result into the canonical spectral representation."""++    metadata = TraceMetadata(+        provider=result.metadata.provider or "upload",+        product_id=result.metadata.product_id or result.content_hash,+        title=result.metadata.title or result.label,+        target=result.metadata.target,+        instrument=result.metadata.instrument,+        telescope=result.metadata.telescope,+        ra=result.metadata.ra,+        dec=result.metadata.dec,+        wave_range_nm=None,+        resolving_power=result.metadata.resolving_power,+        wavelength_standard="air" if result.is_air_wavelength else "vacuum",+        flux_units=result.flux_unit or result.metadata.flux_units or "arbitrary",+        pipeline_version=result.metadata.pipeline_version,+        frame=result.metadata.frame,+        radial_velocity_kms=result.metadata.radial_velocity_kms,+        urls=dict(result.metadata.urls),+        citation=result.metadata.citation,+        doi=result.metadata.doi,+        extra=dict(result.metadata.extra),+    )++    provenance = list(result.provenance)++    unit = normalise_wavelength_unit(result.wavelength_unit)+    wavelength_nm = transforms.convert_axis_to_nm(result.wavelength, unit)+    provenance.append(+        ProvenanceEvent(+            step="convert_wavelength_unit",+            parameters={"from": unit, "to": "nm"},+        )+    )++    if result.is_air_wavelength:+        wavelength_nm = transforms.air_to_vacuum(wavelength_nm)+        metadata.wavelength_standard = "vacuum"+        provenance.append(+            ProvenanceEvent(+                step="air_to_vacuum",+                parameters={"method": "edlen1966"},+                note="Converted from stated air wavelengths using Edlén (1966)",+            )+        )++    metadata.wave_range_nm = (+        float(np.nanmin(wavelength_nm)),+        float(np.nanmax(wavelength_nm)),+    )++    canonical = CanonicalSpectrum(+        label=result.label,+        wavelength_vac_nm=np.asarray(wavelength_nm, dtype=float),+        values=np.asarray(result.flux, dtype=float),+        value_mode="flux_density",+        value_unit=result.flux_unit,+        metadata=metadata,+        provenance=provenance,+        source_hash=result.content_hash,+        uncertainties=result.uncertainties,+    )+    return canonical+++def normalise_wavelength_unit(unit: str | None) -> transforms.WavelengthUnit:+    if not unit:+        return "nm"+    unit_lc = unit.lower()+    if unit_lc in {"nm", "nanometer", "nanometers"}:+        return "nm"+    if unit_lc in {"angstrom", "angstroms", "a", "aa"}:+        return "angstrom"+    if unit_lc in {"micron", "microns", "um", "µm"}:+        return "micron"+    if unit_lc in {"wavenumber", "cm-1", "cm^-1"}:+        return "wavenumber"+    # default to nm+    return "nm"+++__all__ = ["canonicalize_ascii", "canonicalize_fits", "normalise_wavelength_unit"]diff --git a/server/ingest/fits_loader.py b/server/ingest/fits_loader.pynew file mode 100644index 0000000000000000000000000000000000000000..b8fb1f01b18ee4b9d56f0d46601dce73c761480e--- /dev/null+++ b/server/ingest/fits_loader.py@@ -0,0 +1,356 @@+"""FITS spectrum ingestion utilities."""++from __future__ import annotations++import hashlib+import io+import numbers+from collections.abc import Iterable+from contextlib import suppress+from dataclasses import dataclass+from pathlib import Path+from typing import BinaryIO, Literal++import numpy as np+from astropy.io import fits++from server.models import ProvenanceEvent, TraceMetadata++_WAVE_COLUMN_ALIASES = {+    "wavelength",+    "wavelength_nm",+    "lambda",+    "lambda_nm",+    "wave",+    "wl",+    "lam",+    "loglam",+}++_LOG_WAVE_COLUMNS = {"loglam"}++_FLUX_COLUMN_ALIASES = {+    "flux",+    "flux_density",+    "intensity",+    "flux_erg",+    "flux_jy",+}++_UNCERTAINTY_ALIASES = {"uncertainty", "unc", "sigma", "error", "err"}++_UNCERTAINTY_HDU_NAMES = {"ERR", "ERROR", "UNC", "UNCERTAINTY", "SIG", "SIGMA"}+++@dataclass(slots=True)+class FITSIngestResult:+    """Parsed FITS spectrum and accompanying metadata."""++    label: str+    wavelength: np.ndarray+    wavelength_unit: str+    flux: np.ndarray+    flux_unit: str | None+    uncertainties: np.ndarray | None+    metadata: TraceMetadata+    provenance: list[ProvenanceEvent]+    is_air_wavelength: bool+    content_hash: str+++class FITSIngestError(RuntimeError):+    """Raised when FITS ingestion fails."""+++def load_fits_spectrum(+    source: str | Path | BinaryIO, *, filename: str | None = None+) -> FITSIngestResult:+    """Load a 1D spectrum from a FITS file into the ingest model."""++    data_bytes, source_name = _read_source(source, filename)+    if not data_bytes:+        raise FITSIngestError("Empty FITS payload provided")++    content_hash = hashlib.sha256(data_bytes).hexdigest()++    try:+        with fits.open(io.BytesIO(data_bytes), memmap=False) as hdul:+            index, spectrum_hdu = _select_spectrum_hdu(hdul)+            wavelength, wave_unit, wcs_params = _extract_wavelength(spectrum_hdu)+            flux, flux_unit = _extract_flux(spectrum_hdu)+            uncertainties = _extract_uncertainties(hdul, index, flux.size)+            is_air = _detect_air_wavelength(spectrum_hdu.header, wave_unit)+            metadata = _build_metadata(spectrum_hdu.header)+    except OSError as exc:  # pragma: no cover - astropy error handling+        raise FITSIngestError(f"Failed to read FITS file: {exc}") from exc++    metadata.extra.setdefault("wcs", wcs_params)+    provenance = [+        ProvenanceEvent(+            step="ingest_fits",+            parameters={+                "filename": source_name,+                "hdu_index": index,+                "extname": spectrum_hdu.name.strip() or "PRIMARY",+                "flux_unit": flux_unit or metadata.flux_units or "unknown",+                "wavelength_unit": wave_unit,+                "is_air": is_air,+                "hash": content_hash,+                "wcs": wcs_params,+            },+        )+    ]++    label = metadata.target or Path(source_name).stem or "FITS Spectrum"++    return FITSIngestResult(+        label=label,+        wavelength=wavelength,+        wavelength_unit=wave_unit,+        flux=flux,+        flux_unit=flux_unit or metadata.flux_units,+        uncertainties=uncertainties,+        metadata=metadata,+        provenance=provenance,+        is_air_wavelength=is_air,+        content_hash=content_hash,+    )+++def _read_source(source: str | Path | BinaryIO, filename: str | None) -> tuple[bytes, str]:+    if isinstance(source, str) or isinstance(source, Path):+        path = Path(source)+        return path.read_bytes(), path.name+    raw = source.read()+    if hasattr(source, "seek"):+        with suppress(OSError, ValueError):+            source.seek(0)+    name = filename or getattr(source, "name", "stream")+    return raw, str(name)+++def _select_spectrum_hdu(hdul: fits.HDUList) -> tuple[int, fits.hdu.base.ExtensionHDU]:+    for index, hdu in enumerate(hdul):+        data = hdu.data+        if data is None:+            continue+        if isinstance(data, fits.FITS_rec):+            columns = _normalise_names(hdu.columns.names or [])+            if columns & _FLUX_COLUMN_ALIASES:+                return index, hdu+        else:+            array = np.asarray(data)+            if array.ndim == 1 or (array.ndim == 2 and 1 in array.shape):+                return index, hdu+    raise FITSIngestError("No 1D spectral data found in FITS file")+++def _extract_flux(hdu: fits.hdu.base.ExtensionHDU) -> tuple[np.ndarray, str | None]:+    data = hdu.data+    if isinstance(data, fits.FITS_rec):+        columns = list(hdu.columns.names or [])+        column = _match_column(columns, _FLUX_COLUMN_ALIASES)+        if column is None:+            raise FITSIngestError("FITS table is missing a flux column")+        flux_unit = _clean_unit(hdu.columns[column].unit)+        flux = np.asarray(data[column], dtype=float)+        return flux, flux_unit+    flux = np.asarray(data, dtype=float).reshape(-1)+    flux_unit = _clean_unit(hdu.header.get("BUNIT"))+    return flux, flux_unit+++def _extract_wavelength(+    hdu: fits.hdu.base.ExtensionHDU,+) -> tuple[np.ndarray, str, dict[str, float | str | None]]:+    data = hdu.data+    header = hdu.header+    if isinstance(data, fits.FITS_rec):+        columns = list(hdu.columns.names or [])+        column = _match_column(columns, _WAVE_COLUMN_ALIASES)+        if column is None:+            raise FITSIngestError("FITS table is missing a wavelength column")+        values = np.asarray(data[column], dtype=float)+        unit = _clean_unit(hdu.columns[column].unit) or _clean_unit(header.get("CUNIT1"))+        if column.lower() in _LOG_WAVE_COLUMNS:+            values = np.power(10.0, values)+            unit = unit or "angstrom"+        return values, unit or "unknown", {}++    array = np.asarray(data, dtype=float).reshape(-1)+    wcs_params = _wcs_parameters(header)+    if wcs_params is None:+        raise FITSIngestError("FITS image lacks WCS dispersion keywords")+    crval1, cdelt1, crpix1, ctype1, unit = wcs_params+    pixels = np.arange(array.size, dtype=float)+    wavelengths = crval1 + (pixels + 1 - crpix1) * cdelt1+    ctype_upper = ctype1.upper()+    if ctype_upper.startswith("LOG"):+        wavelengths = np.power(10.0, wavelengths)+        unit = unit or "angstrom"+    return (+        wavelengths,+        unit or "unknown",+        {+            "CRVAL1": crval1,+            "CDELT1": cdelt1,+            "CRPIX1": crpix1,+            "CTYPE1": ctype1,+            "CUNIT1": unit,+        },+    )+++def _extract_uncertainties(hdul: fits.HDUList, skip_index: int, length: int) -> np.ndarray | None:+    for index, hdu in enumerate(hdul):+        if index == skip_index:+            continue+        data = hdu.data+        if data is None:+            continue+        if isinstance(data, fits.FITS_rec):+            columns = list(hdu.columns.names or [])+            column = _match_column(columns, _UNCERTAINTY_ALIASES)+            if column is not None:+                uncertainties = np.asarray(data[column], dtype=float).reshape(-1)+                if uncertainties.size == length:+                    return uncertainties+        else:+            if (hdu.name or "").strip().upper() not in _UNCERTAINTY_HDU_NAMES:+                continue+            uncertainties = np.asarray(data, dtype=float).reshape(-1)+            if uncertainties.size == length:+                return uncertainties+    return None+++def _detect_air_wavelength(header: fits.Header, unit: str | None) -> bool:+    ctype1 = str(header.get("CTYPE1", "")).strip().upper()+    if ctype1.startswith("AWAV"):+        return True+    if ctype1.startswith("WAVE"):+        return False+    airorvac = str(header.get("AIRORVAC", "")).strip().lower()+    if airorvac == "air":+        return True+    if airorvac == "vac" or airorvac == "vacuum":+        return False+    vacuum_flag = str(header.get("VACUUM", "")).strip().upper()+    if vacuum_flag == "F":+        return True+    if vacuum_flag == "T":+        return False+    return bool(unit and "air" in unit.lower())+++def _build_metadata(header: fits.Header) -> TraceMetadata:+    specsys_raw = _clean_str(header.get("SPECSYS"))+    metadata = TraceMetadata(+        target=_clean_str(header.get("OBJECT")),+        instrument=_clean_str(header.get("INSTRUME")),+        telescope=_clean_str(header.get("TELESCOP")),+        flux_units=_clean_unit(header.get("BUNIT")),+        pipeline_version=_clean_str(+            header.get("PIPEVER")+            or header.get("PROCVERS")+            or header.get("VERSION")+            or header.get("PIPELINE")+        ),+        frame=_normalise_frame(specsys_raw),+        radial_velocity_kms=_safe_float(+            header.get("VRAD") or header.get("VHELIO") or header.get("RADVEL")+        ),+    )+    metadata.ra = _safe_float(header.get("RA") or header.get("OBJRA"))+    metadata.dec = _safe_float(header.get("DEC") or header.get("OBJDEC"))+    metadata.resolving_power = _safe_float(+        header.get("R") or header.get("RVAL") or header.get("RESOLUT")+    )+    metadata.extra.update(+        {+            "observer": _clean_str(header.get("OBSERVER")),+            "exposure_time": _safe_float(header.get("EXPTIME")),+            "date_obs": _clean_str(header.get("DATE-OBS")),+        }+    )+    if specsys_raw and metadata.frame is None:+        metadata.extra["original_frame"] = specsys_raw+    return metadata+++def _match_column(columns: Iterable[str], aliases: set[str]) -> str | None:+    for name in columns:+        if name is None:+            continue+        if name.lower() in aliases:+            return name+    return None+++def _normalise_names(columns: Iterable[str]) -> set[str]:+    return {name.lower() for name in columns if name}+++def _clean_unit(unit: str | None) -> str | None:+    if unit is None:+        return None+    text = str(unit).strip()+    return text or None+++def _clean_str(value: str | None) -> str | None:+    if value is None:+        return None+    text = str(value).strip()+    return text or None+++def _safe_float(value: object | None) -> float | None:+    if value is None:+        return None+    if isinstance(value, numbers.Real):+        return float(value)+    try:+        return float(str(value).strip())+    except (TypeError, ValueError):+        return None+++def _normalise_frame(+    value: str | None,+) -> Literal["topocentric", "heliocentric", "barycentric", "unknown", "none"] | None:+    if value is None:+        return None+    lookup: dict[str, Literal["topocentric", "heliocentric", "barycentric"]] = {+        "TOPOCENT": "topocentric",+        "GEOCENTR": "topocentric",+        "HELIOCEN": "heliocentric",+        "HELIOCENTRIC": "heliocentric",+        "BARYCENT": "barycentric",+        "BARYCENTRIC": "barycentric",+    }+    upper = value.upper()+    if upper in lookup:+        return lookup[upper]+    lower = value.lower()+    if lower == "unknown":+        return "unknown"+    if lower == "none":+        return "none"+    return None+++def _wcs_parameters(+    header: fits.Header,+) -> tuple[float, float, float, str, str | None] | None:+    crval1 = _safe_float(header.get("CRVAL1"))+    cdelt1 = _safe_float(header.get("CDELT1") or header.get("CD1_1"))+    crpix1 = _safe_float(header.get("CRPIX1")) or 1.0+    ctype1 = _clean_str(header.get("CTYPE1")) or ""+    unit = _clean_unit(header.get("CUNIT1") or header.get("XUNIT"))+    if crval1 is None or cdelt1 is None:+        return None+    return crval1, cdelt1, crpix1, ctype1, unit+++__all__ = ["FITSIngestError", "FITSIngestResult", "load_fits_spectrum"]diff --git a/server/math/__init__.py b/server/math/__init__.pynew file mode 100644index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391diff --git a/server/math/differential.py b/server/math/differential.pynew file mode 100644index 0000000000000000000000000000000000000000..841f0b1783d439df6b071170348a1c0fdd5a2e5a--- /dev/null+++ b/server/math/differential.py@@ -0,0 +1,148 @@+"""Differential spectral operations with safeguards."""++from __future__ import annotations++from dataclasses import dataclass++import numpy as np++from server.models import CanonicalSpectrum, ProvenanceEvent, TraceMetadata++EPSILON_DEFAULT = 1e-8+++@dataclass(slots=True)+class DifferentialProduct:+    spectrum: CanonicalSpectrum+    operation: str+++def _resample(target_wavelengths: np.ndarray, source: CanonicalSpectrum) -> np.ndarray:+    return np.interp(+        target_wavelengths, source.wavelength_vac_nm, source.values, left=np.nan, right=np.nan+    )+++def _combine_uncertainties_sum(a: np.ndarray | None, b: np.ndarray | None) -> np.ndarray | None:+    if a is None and b is None:+        return None+    a_arr = np.asarray(a, dtype=float) if a is not None else 0.0+    b_arr = np.asarray(b, dtype=float) if b is not None else 0.0+    return np.sqrt(np.square(a_arr) + np.square(b_arr))+++def _combine_uncertainties_ratio(+    result: np.ndarray, a: np.ndarray | None, b: np.ndarray | None+) -> np.ndarray | None:+    if a is None and b is None:+        return None+    result_arr = np.asarray(result, dtype=float)+    a_arr = np.asarray(a, dtype=float) if a is not None else np.zeros_like(result_arr)+    b_arr = np.asarray(b, dtype=float) if b is not None else np.zeros_like(result_arr)+    with np.errstate(divide="ignore", invalid="ignore"):+        rel = np.zeros_like(result_arr)+        mask = result_arr != 0+        rel[mask] = np.sqrt(+            np.square(a_arr[mask] / np.clip(a_arr[mask], EPSILON_DEFAULT, None))+            + np.square(b_arr[mask] / np.clip(b_arr[mask], EPSILON_DEFAULT, None))+        )+    return np.where(mask, rel * np.abs(result_arr), np.nan)+++def _create_metadata(base: CanonicalSpectrum, label_suffix: str) -> TraceMetadata:+    metadata = TraceMetadata(+        provider="derived",+        product_id=f"{base.metadata.product_id or base.label}-{label_suffix}",+        title=f"{base.label} {label_suffix}",+        target=base.metadata.target,+        ra=base.metadata.ra,+        dec=base.metadata.dec,+        resolving_power=base.metadata.resolving_power,+        wavelength_standard="vacuum",+        flux_units=base.metadata.flux_units,+        pipeline_version=base.metadata.pipeline_version,+        frame=base.metadata.frame,+        radial_velocity_kms=base.metadata.radial_velocity_kms,+        urls={},+        citation=base.metadata.citation,+        doi=base.metadata.doi,+        extra={"derived_from": base.metadata.to_dict()},+    )+    return metadata+++def _identical(a: CanonicalSpectrum, b: CanonicalSpectrum, atol: float = 1e-12) -> bool:+    if a.wavelength_vac_nm.shape != b.wavelength_vac_nm.shape:+        return False+    if not np.allclose(a.wavelength_vac_nm, b.wavelength_vac_nm, atol=atol):+        return False+    if not np.allclose(a.values, b.values, atol=atol):+        return False+    return True+++def subtract(+    a: CanonicalSpectrum, b: CanonicalSpectrum, *, epsilon: float = EPSILON_DEFAULT+) -> DifferentialProduct | None:+    if _identical(a, b):+        return None++    target_grid = a.wavelength_vac_nm+    b_resampled = _resample(target_grid, b)+    difference = np.asarray(a.values, dtype=float) - np.asarray(b_resampled, dtype=float)++    uncertainties = _combine_uncertainties_sum(a.uncertainties, b.uncertainties)++    metadata = _create_metadata(a, "minus")+    provenance = a.provenance + [+        ProvenanceEvent(+            step="differential_subtract", parameters={"epsilon": epsilon, "other": b.label}+        )+    ]+    spectrum = CanonicalSpectrum(+        label=f"{a.label}-{b.label}",+        wavelength_vac_nm=target_grid,+        values=difference,+        value_mode=a.value_mode,+        value_unit=a.value_unit,+        metadata=metadata,+        provenance=provenance,+        source_hash=None,+        uncertainties=uncertainties,+    )+    return DifferentialProduct(spectrum=spectrum, operation="subtract")+++def divide(+    a: CanonicalSpectrum, b: CanonicalSpectrum, *, epsilon: float = EPSILON_DEFAULT+) -> DifferentialProduct | None:+    if _identical(a, b):+        return None++    target_grid = a.wavelength_vac_nm+    b_resampled = _resample(target_grid, b)+    denominator = np.asarray(b_resampled, dtype=float)+    result = np.asarray(a.values, dtype=float) / np.clip(denominator, epsilon, None)+    uncertainties = _combine_uncertainties_ratio(result, a.uncertainties, b.uncertainties)++    metadata = _create_metadata(a, "divided")+    provenance = a.provenance + [+        ProvenanceEvent(+            step="differential_divide", parameters={"epsilon": epsilon, "other": b.label}+        )+    ]+    spectrum = CanonicalSpectrum(+        label=f"{a.label}/{b.label}",+        wavelength_vac_nm=target_grid,+        values=result,+        value_mode="relative_intensity",+        value_unit=None,+        metadata=metadata,+        provenance=provenance,+        source_hash=None,+        uncertainties=uncertainties,+    )+    return DifferentialProduct(spectrum=spectrum, operation="divide")+++__all__ = ["DifferentialProduct", "divide", "subtract"]diff --git a/server/math/resolution.py b/server/math/resolution.pynew file mode 100644index 0000000000000000000000000000000000000000..36e8e2526d59e78e2830886ecc614af6ef283113--- /dev/null+++ b/server/math/resolution.py@@ -0,0 +1,87 @@+"""Resolution matching helpers."""++from __future__ import annotations++from dataclasses import dataclass++import numpy as np+from scipy.ndimage import gaussian_filter1d+++@dataclass(slots=True)+class ResolutionMatchResult:+    flux: np.ndarray+    kernel_sigma_px: float+    target_resolution: float+++def _median_spacing(wavelength_nm: np.ndarray) -> float:+    diffs = np.diff(np.asarray(wavelength_nm, dtype=float))+    diffs = diffs[np.isfinite(diffs) & (diffs > 0)]+    if diffs.size == 0:+        return 0.0+    return float(np.median(diffs))+++def match_resolution(+    wavelength_nm: np.ndarray,+    flux: np.ndarray,+    current_resolution: float | None,+    target_resolution: float,+) -> ResolutionMatchResult:+    """Convolve a high-resolution spectrum to the requested resolving power."""++    if target_resolution <= 0:+        raise ValueError("target_resolution must be positive")++    if current_resolution is not None and target_resolution >= current_resolution:+        return ResolutionMatchResult(+            flux=np.asarray(flux, dtype=float),+            kernel_sigma_px=0.0,+            target_resolution=target_resolution,+        )++    wavelength_nm = np.asarray(wavelength_nm, dtype=float)+    flux = np.asarray(flux, dtype=float)++    spacing = _median_spacing(wavelength_nm)+    if spacing <= 0:+        return ResolutionMatchResult(+            flux=flux, kernel_sigma_px=0.0, target_resolution=target_resolution+        )++    lam_ref = float(np.median(wavelength_nm))+    current_fwhm = lam_ref / current_resolution if current_resolution else 0.0+    target_fwhm = lam_ref / target_resolution+    kernel_fwhm = np.sqrt(max(target_fwhm**2 - current_fwhm**2, 0.0))+    sigma_nm = kernel_fwhm / (2.0 * np.sqrt(2.0 * np.log(2.0)))+    sigma_px = sigma_nm / spacing+    if not np.isfinite(sigma_px) or sigma_px <= 0:+        return ResolutionMatchResult(+            flux=flux, kernel_sigma_px=0.0, target_resolution=target_resolution+        )++    blurred = gaussian_filter1d(flux, sigma=sigma_px, mode="nearest")+    return ResolutionMatchResult(+        flux=blurred, kernel_sigma_px=float(sigma_px), target_resolution=target_resolution+    )+++def estimate_fwhm(wavelength_nm: np.ndarray, flux: np.ndarray) -> float:+    """Estimate the FWHM of the central peak for validation."""++    wavelength_nm = np.asarray(wavelength_nm, dtype=float)+    flux = np.asarray(flux, dtype=float)+    peak_index = int(np.argmax(flux))+    peak_value = flux[peak_index]+    half_max = peak_value / 2.0+    left_indices = np.where(flux[:peak_index] <= half_max)[0]+    right_indices = np.where(flux[peak_index:] <= half_max)[0]+    if left_indices.size == 0 or right_indices.size == 0:+        return 0.0+    left = wavelength_nm[left_indices[-1]]+    right = wavelength_nm[peak_index + right_indices[0]]+    return float(right - left)+++__all__ = ["ResolutionMatchResult", "estimate_fwhm", "match_resolution"]diff --git a/server/math/rv_frame.py b/server/math/rv_frame.pynew file mode 100644index 0000000000000000000000000000000000000000..2a9e685db8b7b2ca1e984692a93f45c0c42a9111--- /dev/null+++ b/server/math/rv_frame.py@@ -0,0 +1,30 @@+"""Radial-velocity frame corrections."""++from __future__ import annotations++from typing import Literal++import numpy as np++from server.math.transforms import doppler_shift_wavelength++FrameTag = Literal["topocentric", "heliocentric", "barycentric", "unknown", "none"]+++_C = 299_792.458  # km/s+++def shift_to_rest_frame(wavelength_nm: np.ndarray, velocity_kms: float) -> np.ndarray:+    """Remove a radial velocity from observed wavelengths."""++    factor = 1.0 / (1.0 + velocity_kms / _C)+    return np.asarray(wavelength_nm, dtype=float) * factor+++def shift_from_rest_frame(wavelength_nm: np.ndarray, velocity_kms: float) -> np.ndarray:+    """Apply a radial velocity to rest-frame wavelengths."""++    return doppler_shift_wavelength(wavelength_nm, velocity_kms)+++__all__ = ["FrameTag", "shift_from_rest_frame", "shift_to_rest_frame"]diff --git a/server/math/transforms.py b/server/math/transforms.pynew file mode 100644index 0000000000000000000000000000000000000000..a65b3f27681597a016412fe76a0daa8b1d0b4742--- /dev/null+++ b/server/math/transforms.py@@ -0,0 +1,140 @@+"""Spectral unit conversions and intensity family helpers."""++from __future__ import annotations++from typing import Literal++import numpy as np++WavelengthUnit = Literal["nm", "angstrom", "micron", "wavenumber"]+IntensityMode = Literal[+    "flux_density",+    "transmission",+    "absorbance",+    "optical_depth",+    "relative_intensity",+]++_C = 299_792.458  # km / s+++def nm_to_angstrom(wavelength_nm: np.ndarray | float) -> np.ndarray:+    return np.asarray(wavelength_nm, dtype=float) * 10.0+++def angstrom_to_nm(wavelength_angstrom: np.ndarray | float) -> np.ndarray:+    return np.asarray(wavelength_angstrom, dtype=float) / 10.0+++def nm_to_micron(wavelength_nm: np.ndarray | float) -> np.ndarray:+    return np.asarray(wavelength_nm, dtype=float) / 1000.0+++def micron_to_nm(wavelength_micron: np.ndarray | float) -> np.ndarray:+    return np.asarray(wavelength_micron, dtype=float) * 1000.0+++def nm_to_wavenumber(wavelength_nm: np.ndarray | float) -> np.ndarray:+    return 1e7 / np.asarray(wavelength_nm, dtype=float)+++def wavenumber_to_nm(wavenumber: np.ndarray | float) -> np.ndarray:+    return 1e7 / np.asarray(wavenumber, dtype=float)+++def convert_axis_from_nm(values_nm: np.ndarray, to_unit: WavelengthUnit) -> np.ndarray:+    if to_unit == "nm":+        return np.asarray(values_nm, dtype=float)+    if to_unit == "angstrom":+        return nm_to_angstrom(values_nm)+    if to_unit == "micron":+        return nm_to_micron(values_nm)+    if to_unit == "wavenumber":+        return nm_to_wavenumber(values_nm)+    raise ValueError(f"Unsupported unit: {to_unit}")+++def convert_axis_to_nm(values: np.ndarray, from_unit: WavelengthUnit) -> np.ndarray:+    if from_unit == "nm":+        return np.asarray(values, dtype=float)+    if from_unit == "angstrom":+        return angstrom_to_nm(values)+    if from_unit == "micron":+        return micron_to_nm(values)+    if from_unit == "wavenumber":+        return wavenumber_to_nm(values)+    raise ValueError(f"Unsupported unit: {from_unit}")+++def refractive_index_edlen(wavelength_nm: np.ndarray | float) -> np.ndarray:+    """Refractive index of standard air using the Edlén 1966 parameterization."""++    wavelength_um = np.asarray(wavelength_nm, dtype=float) / 1000.0+    sigma2 = (1.0 / wavelength_um) ** 2+    term = 8342.13 + (2406030.0 / (130.0 - sigma2)) + (15997.0 / (38.9 - sigma2))+    return 1.0 + term * 1e-8+++def air_to_vacuum(wavelength_air_nm: np.ndarray | float) -> np.ndarray:+    n = refractive_index_edlen(wavelength_air_nm)+    return np.asarray(wavelength_air_nm, dtype=float) * n+++def vacuum_to_air(wavelength_vacuum_nm: np.ndarray | float) -> np.ndarray:+    n = refractive_index_edlen(wavelength_vacuum_nm)+    return np.asarray(wavelength_vacuum_nm, dtype=float) / n+++def transmission_to_absorbance(transmission: np.ndarray | float) -> np.ndarray:+    transmission_arr = np.clip(np.asarray(transmission, dtype=float), 1e-12, 1.0)+    return -np.log10(transmission_arr)+++def absorbance_to_transmission(absorbance: np.ndarray | float) -> np.ndarray:+    absorbance_arr = np.asarray(absorbance, dtype=float)+    return np.power(10.0, -absorbance_arr)+++def transmission_to_optical_depth(transmission: np.ndarray | float) -> np.ndarray:+    transmission_arr = np.clip(np.asarray(transmission, dtype=float), 1e-12, 1.0)+    return -np.log(transmission_arr)+++def optical_depth_to_transmission(optical_depth: np.ndarray | float) -> np.ndarray:+    optical_depth_arr = np.asarray(optical_depth, dtype=float)+    return np.exp(-optical_depth_arr)+++def absorbance_to_optical_depth(absorbance: np.ndarray | float) -> np.ndarray:+    return transmission_to_optical_depth(absorbance_to_transmission(absorbance))+++def optical_depth_to_absorbance(optical_depth: np.ndarray | float) -> np.ndarray:+    return transmission_to_absorbance(optical_depth_to_transmission(optical_depth))+++def doppler_shift_wavelength(wavelength_nm: np.ndarray, velocity_kms: float) -> np.ndarray:+    factor = 1.0 + velocity_kms / _C+    return np.asarray(wavelength_nm, dtype=float) * factor+++__all__ = [+    "IntensityMode",+    "WavelengthUnit",+    "air_to_vacuum",+    "absorbance_to_optical_depth",+    "absorbance_to_transmission",+    "convert_axis_from_nm",+    "convert_axis_to_nm",+    "doppler_shift_wavelength",+    "nm_to_angstrom",+    "nm_to_micron",+    "nm_to_wavenumber",+    "optical_depth_to_absorbance",+    "optical_depth_to_transmission",+    "refractive_index_edlen",+    "transmission_to_absorbance",+    "transmission_to_optical_depth",+    "vacuum_to_air",+    "wavenumber_to_nm",+]diff --git a/server/models.py b/server/models.pynew file mode 100644index 0000000000000000000000000000000000000000..a3900041613b4f579f566bb6c557191994b7dfce--- /dev/null+++ b/server/models.py@@ -0,0 +1,184 @@+"""Shared data models for spectra and provenance."""++from __future__ import annotations++from dataclasses import dataclass, field+from datetime import UTC, datetime+from typing import Any, Literal++import numpy as np++ValueMode = Literal[+    "flux_density",+    "relative_intensity",+    "transmission",+    "absorbance",+    "optical_depth",+]+++@dataclass(slots=True)+class ProvenanceEvent:+    """Record a deterministic transform applied to a spectrum."""++    step: str+    parameters: dict[str, Any] = field(default_factory=dict)+    note: str | None = None+    timestamp: datetime = field(default_factory=lambda: datetime.now(UTC))++    def to_dict(self) -> dict[str, Any]:+        return {+            "step": self.step,+            "parameters": self.parameters,+            "note": self.note,+            "timestamp": self.timestamp.isoformat(),+        }++    @classmethod+    def from_dict(cls, payload: dict[str, Any]) -> ProvenanceEvent:+        timestamp_raw = payload.get("timestamp")+        timestamp = (+            datetime.fromisoformat(timestamp_raw)+            if isinstance(timestamp_raw, str)+            else datetime.now(UTC)+        )+        return cls(+            step=payload["step"],+            parameters=dict(payload.get("parameters", {})),+            note=payload.get("note"),+            timestamp=timestamp,+        )+++@dataclass(slots=True)+class TraceMetadata:+    """Metadata associated with a spectrum trace."""++    provider: str | None = None+    product_id: str | None = None+    title: str | None = None+    target: str | None = None+    instrument: str | None = None+    telescope: str | None = None+    ra: float | None = None+    dec: float | None = None+    wave_range_nm: tuple[float, float] | None = None+    resolving_power: float | None = None+    wavelength_standard: Literal["air", "vacuum", "unknown"] | None = None+    flux_units: str | None = None+    pipeline_version: str | None = None+    frame: Literal["topocentric", "heliocentric", "barycentric", "unknown", "none"] | None = None+    radial_velocity_kms: float | None = None+    urls: dict[str, str] = field(default_factory=dict)+    citation: str | None = None+    doi: str | None = None+    extra: dict[str, Any] = field(default_factory=dict)++    def to_dict(self) -> dict[str, Any]:+        return {+            "provider": self.provider,+            "product_id": self.product_id,+            "title": self.title,+            "target": self.target,+            "instrument": self.instrument,+            "telescope": self.telescope,+            "ra": self.ra,+            "dec": self.dec,+            "wave_range_nm": list(self.wave_range_nm) if self.wave_range_nm else None,+            "resolving_power": self.resolving_power,+            "wavelength_standard": self.wavelength_standard,+            "flux_units": self.flux_units,+            "pipeline_version": self.pipeline_version,+            "frame": self.frame,+            "radial_velocity_kms": self.radial_velocity_kms,+            "urls": self.urls,+            "citation": self.citation,+            "doi": self.doi,+            "extra": self.extra,+        }++    @classmethod+    def from_dict(cls, payload: dict[str, Any]) -> TraceMetadata:+        wave_range = payload.get("wave_range_nm")+        wave_range_tuple = None+        if isinstance(wave_range, list | tuple) and len(wave_range) == 2:+            wave_range_tuple = (float(wave_range[0]), float(wave_range[1]))+        return cls(+            provider=payload.get("provider"),+            product_id=payload.get("product_id"),+            title=payload.get("title"),+            target=payload.get("target"),+            instrument=payload.get("instrument"),+            telescope=payload.get("telescope"),+            ra=payload.get("ra"),+            dec=payload.get("dec"),+            wave_range_nm=wave_range_tuple,+            resolving_power=payload.get("resolving_power"),+            wavelength_standard=payload.get("wavelength_standard"),+            flux_units=payload.get("flux_units"),+            pipeline_version=payload.get("pipeline_version"),+            frame=payload.get("frame"),+            radial_velocity_kms=payload.get("radial_velocity_kms"),+            urls=dict(payload.get("urls", {})),+            citation=payload.get("citation"),+            doi=payload.get("doi"),+            extra=dict(payload.get("extra", {})),+        )+++@dataclass(slots=True)+class CanonicalSpectrum:+    """Spectrum data normalized to the canonical wavelength baseline."""++    label: str+    wavelength_vac_nm: np.ndarray+    values: np.ndarray+    value_mode: ValueMode+    value_unit: str | None+    metadata: TraceMetadata+    provenance: list[ProvenanceEvent] = field(default_factory=list)+    source_hash: str | None = None+    uncertainties: np.ndarray | None = None++    def to_manifest_entry(self) -> dict[str, Any]:+        return {+            "label": self.label,+            "wavelength_vac_nm": self.wavelength_vac_nm.tolist(),+            "values": self.values.tolist(),+            "value_mode": self.value_mode,+            "value_unit": self.value_unit,+            "metadata": self.metadata.to_dict(),+            "provenance": [event.to_dict() for event in self.provenance],+            "source_hash": self.source_hash,+            "uncertainties": (+                self.uncertainties.tolist() if self.uncertainties is not None else None+            ),+        }++    @classmethod+    def from_manifest_entry(cls, payload: dict[str, Any]) -> CanonicalSpectrum:+        provenance_payload = payload.get("provenance", [])+        provenance = [ProvenanceEvent.from_dict(item) for item in provenance_payload]+        uncertainties_raw = payload.get("uncertainties")+        uncertainties = None+        if uncertainties_raw is not None:+            uncertainties = np.asarray(uncertainties_raw, dtype=float)+        return cls(+            label=payload["label"],+            wavelength_vac_nm=np.asarray(payload["wavelength_vac_nm"], dtype=float),+            values=np.asarray(payload["values"], dtype=float),+            value_mode=payload.get("value_mode", "flux_density"),+            value_unit=payload.get("value_unit"),+            metadata=TraceMetadata.from_dict(payload.get("metadata", {})),+            provenance=provenance,+            source_hash=payload.get("source_hash"),+            uncertainties=uncertainties,+        )+++__all__ = [+    "CanonicalSpectrum",+    "ProvenanceEvent",+    "TraceMetadata",+    "ValueMode",+]diff --git a/server/overlays/__init__.py b/server/overlays/__init__.pynew file mode 100644index 0000000000000000000000000000000000000000..e69de29bb2d1d6434b8b29ae775ad8c2e48c5391diff --git a/server/overlays/lines.py b/server/overlays/lines.pynew file mode 100644index 0000000000000000000000000000000000000000..3c22009f818f8ccf98e20de9b28c89657a098035--- /dev/null+++ b/server/overlays/lines.py@@ -0,0 +1,123 @@+"""Atomic and molecular line overlay utilities."""++from __future__ import annotations++from collections.abc import Iterable+from dataclasses import dataclass+from pathlib import Path++import numpy as np+import pandas as pd++from server.math.transforms import doppler_shift_wavelength++_DATA_DIR = Path(__file__).resolve().parents[2] / "data" / "examples"+++@dataclass(slots=True)+class LineEntry:+    species: str+    wavelength_nm: float+    relative_intensity: float+    log_gf: float | None = None+    Aki: float | None = None+++@dataclass(slots=True)+class ScaledLine:+    species: str+    wavelength_nm: float+    display_height: float+    relative_intensity: float+    metadata: dict+++class LineCatalog:+    """Load static line data from local fixtures."""++    def __init__(self, path: Path | None = None) -> None:+        self._path = path or (_DATA_DIR / "nist_fe_lines.csv")+        if not self._path.exists():+            raise FileNotFoundError(f"Line catalog not found: {self._path}")+        self._frame = pd.read_csv(self._path)++    def species(self) -> list[str]:+        return sorted({str(value).strip() for value in self._frame["species"].unique()})++    def lines_for_species(self, species: str) -> list[LineEntry]:+        subset = self._frame[self._frame["species"].str.lower() == species.lower()]+        entries = []+        for _, row in subset.iterrows():+            entries.append(+                LineEntry(+                    species=row["species"],+                    wavelength_nm=float(row["wavelength_nm"]),+                    relative_intensity=float(row["relative_intensity"]),+                    log_gf=float(row["log_gf"]) if not pd.isna(row["log_gf"]) else None,+                    Aki=float(row["Aki"]) if not pd.isna(row["Aki"]) else None,+                )+            )+        return entries+++def scale_lines(+    entries: Iterable[LineEntry],+    *,+    mode: str = "relative",+    gamma: float = 1.0,+    min_relative_intensity: float = 0.0,+) -> list[ScaledLine]:+    entries_list = list(entries)+    if not entries_list:+        return []++    intensities = np.array([entry.relative_intensity for entry in entries_list], dtype=float)+    if np.all(intensities <= 0):+        intensities = np.ones_like(intensities)++    if mode == "relative":+        normaliser = np.max(intensities)+    elif mode == "quantile":+        normaliser = np.quantile(intensities, 0.99)+    else:+        raise ValueError(f"Unsupported scaling mode: {mode}")++    if normaliser <= 0:+        normaliser = np.max(intensities)+    scaled = np.clip(intensities / normaliser, 0.0, 1.0)+    scaled = np.power(scaled, gamma)++    result: list[ScaledLine] = []+    for entry, relative_height, scale in zip(entries_list, intensities, scaled, strict=False):+        if normaliser > 0 and (relative_height / normaliser) < min_relative_intensity:+            continue+        result.append(+            ScaledLine(+                species=entry.species,+                wavelength_nm=entry.wavelength_nm,+                display_height=float(scale),+                relative_intensity=float(relative_height),+                metadata={"log_gf": entry.log_gf, "Aki": entry.Aki},+            )+        )+    return result+++def apply_velocity_shift(lines: Iterable[ScaledLine], velocity_kms: float) -> list[ScaledLine]:+    shifted: list[ScaledLine] = []+    for line in lines:+        shifted.append(+            ScaledLine(+                species=line.species,+                wavelength_nm=float(+                    doppler_shift_wavelength(np.array([line.wavelength_nm]), velocity_kms)[0]+                ),+                display_height=line.display_height,+                relative_intensity=line.relative_intensity,+                metadata=line.metadata,+            )+        )+    return shifted+++__all__ = ["LineCatalog", "LineEntry", "ScaledLine", "apply_velocity_shift", "scale_lines"]diff --git a/tests/test_ascii_loader.py b/tests/test_ascii_loader.pynew file mode 100644index 0000000000000000000000000000000000000000..3fa3da10aabca23aa6316761e4cfde236330618b--- /dev/null+++ b/tests/test_ascii_loader.py@@ -0,0 +1,29 @@+from __future__ import annotations++from pathlib import Path++from app.state.session import AppSessionState+from server.ingest.ascii_loader import load_ascii_spectrum+from server.ingest.canonicalize import canonicalize_ascii+++def test_ascii_loader_parses_units(tmp_path: Path) -> None:+    fixture = Path("data/examples/example_spectrum.csv")+    content = fixture.read_bytes()+    result = load_ascii_spectrum(content, fixture.name)+    assert result.wavelength_unit == "nm"+    assert result.metadata.target == "Example Star"+    canonical = canonicalize_ascii(result)+    assert canonical.metadata.flux_units == "arb"+++def test_session_deduplication() -> None:+    fixture = Path("data/examples/example_spectrum.csv")+    result = load_ascii_spectrum(fixture.read_bytes(), fixture.name)+    canonical = canonicalize_ascii(result)+    session = AppSessionState()+    added, trace_id = session.register_trace(canonical)+    assert added+    added_again, duplicate_id = session.register_trace(canonical)+    assert not added_again+    assert duplicate_id == trace_iddiff --git a/tests/test_canonicalize.py b/tests/test_canonicalize.pynew file mode 100644index 0000000000000000000000000000000000000000..7a17d75ea5d9f3dc0cf4e845b7fe064a19899c6c--- /dev/null+++ b/tests/test_canonicalize.py@@ -0,0 +1,13 @@+from __future__ import annotations++from server.ingest.ascii_loader import load_ascii_spectrum+from server.ingest.canonicalize import canonicalize_ascii+++def test_air_to_vacuum_provenance() -> None:+    content = b"Wavelength_air (angstrom),Flux\n" b"5000,1.0\n" b"5001,1.1\n"+    ascii_result = load_ascii_spectrum(content, "air_example.csv")+    canonical = canonicalize_ascii(ascii_result)+    steps = [event.step for event in canonical.provenance]+    assert "air_to_vacuum" in steps+    assert canonical.metadata.wavelength_standard == "vacuum"diff --git a/tests/test_differential.py b/tests/test_differential.pynew file mode 100644index 0000000000000000000000000000000000000000..3b512497087fc812ef283625a0b2f266f8fedbe9--- /dev/null+++ b/tests/test_differential.py@@ -0,0 +1,38 @@+from __future__ import annotations++import numpy as np++from server.math.differential import divide, subtract+from server.models import CanonicalSpectrum, TraceMetadata+++def _make_trace(label: str, values: np.ndarray) -> CanonicalSpectrum:+    return CanonicalSpectrum(+        label=label,+        wavelength_vac_nm=np.linspace(500.0, 501.0, len(values)),+        values=values,+        value_mode="flux_density",+        value_unit="arb",+        metadata=TraceMetadata(provider="test"),+        provenance=[],+        source_hash=label,+    )+++def test_identical_traces_suppressed() -> None:+    values = np.array([1.0, 2.0, 3.0])+    trace_a = _make_trace("A", values)+    trace_b = _make_trace("B", values)+    assert subtract(trace_a, trace_b) is None+    assert divide(trace_a, trace_b) is None+++def test_subtract_and_divide() -> None:+    trace_a = _make_trace("A", np.array([2.0, 4.0, 6.0]))+    trace_b = _make_trace("B", np.array([1.0, 2.0, 3.0]))+    subtraction = subtract(trace_a, trace_b)+    assert subtraction is not None+    np.testing.assert_allclose(subtraction.spectrum.values, [1.0, 2.0, 3.0])+    ratio = divide(trace_a, trace_b)+    assert ratio is not None+    np.testing.assert_allclose(ratio.spectrum.values, [2.0, 2.0, 2.0])diff --git a/tests/test_export_manifest.py b/tests/test_export_manifest.pynew file mode 100644index 0000000000000000000000000000000000000000..36633d9c1992a920f6bafdb7e3f5cf8bef85a161--- /dev/null+++ b/tests/test_export_manifest.py@@ -0,0 +1,40 @@+from __future__ import annotations++import json+from io import BytesIO+from pathlib import Path+from zipfile import ZipFile++from app.state.session import AppSessionState, XAxisUnit+from server.export.manifest import export_session, replay_manifest+from server.ingest.ascii_loader import load_ascii_spectrum+from server.ingest.canonicalize import canonicalize_ascii+++def _session_with_example() -> AppSessionState:+    fixture = Path("data/examples/example_spectrum.csv")+    result = load_ascii_spectrum(fixture.read_bytes(), fixture.name)+    canonical = canonicalize_ascii(result)+    session = AppSessionState()+    session.register_trace(canonical)+    return session+++def test_export_manifest_and_replay() -> None:+    session = _session_with_example()+    bundle = export_session(+        session,+        figure=None,+        app_version="0.1.0b",+        schema_version=2,+        axis_unit=XAxisUnit.NM,+        display_mode=session.display_mode.value,+        overlay_settings={"line_overlay": {"species": "Fe I"}},+        include_png=False,+    )+    with ZipFile(BytesIO(bundle.zip_bytes)) as archive:+        assert "manifest.json" in archive.namelist()+        manifest_payload = json.loads(archive.read("manifest.json"))+    spectra = replay_manifest(manifest_payload)+    assert len(spectra) == 1+    assert spectra[0].label == session.traces[session.trace_order[0]].labeldiff --git a/tests/test_fits_loader.py b/tests/test_fits_loader.pynew file mode 100644index 0000000000000000000000000000000000000000..3615ec1ff267a97500e21ed2a4d91660a5e183c4--- /dev/null+++ b/tests/test_fits_loader.py@@ -0,0 +1,47 @@+from __future__ import annotations++from pathlib import Path++import numpy as np+from astropy.io import fits++from server.ingest.canonicalize import canonicalize_fits+from server.ingest.fits_loader import load_fits_spectrum+++def test_load_fits_spectrum_extracts_metadata() -> None:+    fixture = Path("data/examples/example_spectrum.fits")+    result = load_fits_spectrum(fixture)++    assert result.label == "Example FITS Star"+    assert result.metadata.instrument == "MockSpec"+    assert result.flux_unit == "erg/s/cm2/A"+    assert result.uncertainties is not None+    assert result.uncertainties.shape == result.flux.shape++    canonical = canonicalize_fits(result)+    assert canonical.metadata.wavelength_standard == "vacuum"+    assert np.isclose(canonical.wavelength_vac_nm[0], 500.0)+    assert canonical.metadata.wave_range_nm is not None+++def test_canonicalize_fits_converts_air_wavelengths(tmp_path: Path) -> None:+    flux = np.ones(4, dtype=float)+    header = fits.Header()+    header["CRVAL1"] = 6000.0+    header["CDELT1"] = 2.0+    header["CRPIX1"] = 1.0+    header["CTYPE1"] = "AWAV"+    header["CUNIT1"] = "angstrom"+    header["OBJECT"] = "Air Source"++    path = tmp_path / "air_spectrum.fits"+    fits.HDUList([fits.PrimaryHDU(data=flux, header=header)]).writeto(path)++    result = load_fits_spectrum(path)+    assert result.is_air_wavelength++    canonical = canonicalize_fits(result)+    assert canonical.metadata.wavelength_standard == "vacuum"+    assert canonical.wavelength_vac_nm[0] > 600.0+    assert "air_to_vacuum" in {event.step for event in canonical.provenance}diff --git a/tests/test_lines.py b/tests/test_lines.pynew file mode 100644index 0000000000000000000000000000000000000000..492befd4d98a84c2857f10d1db7d7fe39cff9e4e--- /dev/null+++ b/tests/test_lines.py@@ -0,0 +1,19 @@+from __future__ import annotations++import numpy as np++from server.overlays.lines import LineCatalog, scale_lines+++def test_scale_lines_relative_and_quantile() -> None:+    catalog = LineCatalog()+    entries = catalog.lines_for_species("Fe I")+    relative = scale_lines(entries, mode="relative", gamma=1.0)+    assert relative+    heights = np.array([line.display_height for line in relative])+    assert np.isclose(np.max(heights), 1.0)++    quantile = scale_lines(entries, mode="quantile", gamma=1.0)+    heights_quantile = np.array([line.display_height for line in quantile])+    assert np.max(heights_quantile) <= 1.0+    assert np.median(heights_quantile) > 0diff --git a/tests/test_resolution.py b/tests/test_resolution.pynew file mode 100644index 0000000000000000000000000000000000000000..7efef1c9d73b7c3a18c303a13b602341b362116a--- /dev/null+++ b/tests/test_resolution.py@@ -0,0 +1,20 @@+from __future__ import annotations++import numpy as np++from server.math.resolution import estimate_fwhm, match_resolution+++def _gaussian(wavelengths: np.ndarray, center: float, sigma: float) -> np.ndarray:+    return np.exp(-0.5 * ((wavelengths - center) / sigma) ** 2)+++def test_match_resolution_reduces_resolving_power() -> None:+    wavelengths = np.linspace(500.0, 501.0, 1001)+    flux = _gaussian(wavelengths, 500.5, 0.01)+    result = match_resolution(+        wavelengths, flux, current_resolution=10000.0, target_resolution=2000.0+    )+    fwhm = estimate_fwhm(wavelengths, result.flux)+    expected_fwhm = np.median(wavelengths) / 2000.0+    assert abs(fwhm - expected_fwhm) / expected_fwhm < 0.2diff --git a/tests/test_resolver.py b/tests/test_resolver.pynew file mode 100644index 0000000000000000000000000000000000000000..9631e5a10f06f5cc07c15a57d752d2365a3cabab--- /dev/null+++ b/tests/test_resolver.py@@ -0,0 +1,10 @@+from __future__ import annotations++from server.fetchers import resolver_simbad+++def test_resolver_fixture_m31() -> None:+    result = resolver_simbad.resolve("M 31")+    assert result.canonical_name == "M 31"+    assert result.ra is not None and result.dec is not None+    assert "Messier 31" in result.aliasesdiff --git a/tests/test_transforms.py b/tests/test_transforms.pynew file mode 100644index 0000000000000000000000000000000000000000..0ffa04dffe4d5cb4584d9be02d3b0f4038bd160a--- /dev/null+++ b/tests/test_transforms.py@@ -0,0 +1,31 @@+from __future__ import annotations++import numpy as np++from server.math import transforms+++def test_wavelength_round_trip() -> None:+    values_nm = np.array([100.0, 500.5, 1000.0])+    angstrom = transforms.nm_to_angstrom(values_nm)+    microns = transforms.nm_to_micron(transforms.angstrom_to_nm(angstrom))+    wavenumber = transforms.nm_to_wavenumber(transforms.micron_to_nm(microns))+    result_nm = transforms.wavenumber_to_nm(wavenumber)+    assert np.allclose(values_nm, result_nm)+++def test_air_vacuum_round_trip() -> None:+    air = np.array([400.0, 600.0, 900.0])+    vac = transforms.air_to_vacuum(air)+    back_to_air = transforms.vacuum_to_air(vac)+    assert np.allclose(air, back_to_air, rtol=1e-8, atol=1e-8)+++def test_intensity_family_round_trip() -> None:+    transmission = np.array([1.0, 0.5, 0.1])+    absorbance = transforms.transmission_to_absorbance(transmission)+    recovered = transforms.absorbance_to_transmission(absorbance)+    assert np.allclose(transmission, recovered)+    optical_depth = transforms.transmission_to_optical_depth(transmission)+    recovered_tau = transforms.optical_depth_to_transmission(optical_depth)+    assert np.allclose(transmission, recovered_tau)diff --git a/tools/verifiers/Verify-Atlas.py b/tools/verifiers/Verify-Atlas.pynew file mode 100755index 0000000000000000000000000000000000000000..576751c4c14b255129765a9ee581513b16ccc5e3--- /dev/null+++ b/tools/verifiers/Verify-Atlas.py@@ -0,0 +1,45 @@+#!/usr/bin/env python3+"""Ensure atlas documentation set is present and non-empty."""++from __future__ import annotations++import sys+from pathlib import Path++REQUIRED = [+    "atlas/README.md",+    "atlas/architecture.md",+    "atlas/ui_contract.md",+    "atlas/data_model.md",+    "atlas/ingest_ascii.md",+    "atlas/transforms.md",+    "atlas/export_manifest.md",+    "atlas/fetchers_overview.md",+    "atlas/overlays_lines.md",+    "atlas/performance.md",+    "atlas/testing.md",+]+++def main() -> int:+    root = Path(__file__).resolve().parents[2]+    missing: list[str] = []+    empty: list[str] = []+    for rel in REQUIRED:+        path = root / rel+        if not path.exists():+            missing.append(rel)+            continue+        if not path.read_text(encoding="utf-8").strip():+            empty.append(rel)+    if missing or empty:+        if missing:+            print("Missing atlas files:", ", ".join(missing))+        if empty:+            print("Empty atlas files:", ", ".join(empty))+        return 1+    return 0+++if __name__ == "__main__":  # pragma: no cover - invoked via CLI+    sys.exit(main())diff --git a/tools/verifiers/Verify-Brains.py b/tools/verifiers/Verify-Brains.pynew file mode 100755index 0000000000000000000000000000000000000000..edc05c8c8e469e83156ceada223b59833ab56039--- /dev/null+++ b/tools/verifiers/Verify-Brains.py@@ -0,0 +1,39 @@+#!/usr/bin/env python3+"""Verify brains journal entry exists and has required sections."""++from __future__ import annotations++import re+import sys+from pathlib import Path++PATTERN = re.compile(r"^v\d+\.\d+\.\d+[a-z]__.+\.md$")+REQUIRED_HEADINGS = [+    "## Context",+    "## Changes",+    "## Decisions",+    "## Tests & Evidence",+    "## Regressions Prevented",+    "## Follow-ups",+    "## Checklist",+]+++def main() -> int:+    root = Path(__file__).resolve().parents[2]+    brains_dir = root / "brains"+    entries = sorted([path for path in brains_dir.glob("*.md") if PATTERN.match(path.name)])+    if not entries:+        print("No brains entries found")+        return 1+    latest = entries[-1]+    text = latest.read_text(encoding="utf-8")+    missing = [heading for heading in REQUIRED_HEADINGS if heading not in text]+    if missing:+        print(f"Brains entry {latest.name} missing headings: {', '.join(missing)}")+        return 1+    return 0+++if __name__ == "__main__":  # pragma: no cover+    sys.exit(main())diff --git a/tools/verifiers/Verify-Handoff.py b/tools/verifiers/Verify-Handoff.pynew file mode 100755index 0000000000000000000000000000000000000000..ebc8b7c703c30205d758f0a17f0ce8859f730f84--- /dev/null+++ b/tools/verifiers/Verify-Handoff.py@@ -0,0 +1,53 @@+#!/usr/bin/env python3+"""Validate that a handoff file exists for the current run."""++from __future__ import annotations++import json+import re+import sys+from pathlib import Path++HEADINGS = [+    "## 1) Summary of This Run",+    "## 2) Current State of the Project",+    "## 3) Next Steps (Prioritized)",+    "## 4) Decisions & Rationale",+    "## 5) References",+    "## 6) Quick Start for the Next AI",+]++VERSION_PATTERN = re.compile(r"^(?P<base>\d+\.\d+\.\d+)(?P<letter>[a-z])$")+++def _current_version(root: Path) -> tuple[str, str]:+    payload = json.loads((root / "app" / "config" / "version.json").read_text())+    match = VERSION_PATTERN.match(payload["app_version"])+    if not match:+        raise ValueError("app_version not in expected format")+    return match.group("base"), match.group("letter")+++def main() -> int:+    root = Path(__file__).resolve().parents[2]+    template = root / "handoffs" / "HANDOFF_TEMPLATE.md"+    if not template.exists():+        print("Handoff template missing")+        return 1++    base, letter = _current_version(root)+    target = root / "handoffs" / f"HANDOFF_v{base}({letter}).md"+    if not target.exists():+        print(f"Handoff file missing: {target.name}")+        return 1++    text = target.read_text(encoding="utf-8")+    missing = [heading for heading in HEADINGS if heading not in text]+    if missing:+        print(f"Handoff missing headings: {', '.join(missing)}")+        return 1+    return 0+++if __name__ == "__main__":  # pragma: no cover+    sys.exit(main())diff --git a/tools/verifiers/Verify-PatchNotes.py b/tools/verifiers/Verify-PatchNotes.pynew file mode 100755index 0000000000000000000000000000000000000000..b1fbb52e5e324a42f60afdbbdaf1bfa5e128600d--- /dev/null+++ b/tools/verifiers/Verify-PatchNotes.py@@ -0,0 +1,50 @@+#!/usr/bin/env python3+"""Ensure version artifacts stay in sync."""++from __future__ import annotations++import json+import re+import sys+from pathlib import Path++VERSION_PATTERN = re.compile(r"^(?P<base>\d+\.\d+\.\d+)(?P<letter>[a-z])$")+++def _load_version(root: Path) -> tuple[str, str]:+    payload = json.loads((root / "app" / "config" / "version.json").read_text())+    app_version = payload["app_version"]+    match = VERSION_PATTERN.match(app_version)+    if not match:+        raise ValueError(f"app_version {app_version!r} not in expected format")+    return match.group("base"), match.group("letter")+++def main() -> int:+    root = Path(__file__).resolve().parents[2]+    try:+        base, letter = _load_version(root)+    except ValueError as exc:+        print(exc)+        return 1++    patch_name = root / "PATCH_NOTES" / f"PATCH_NOTES_v{base}({letter}).md"+    if not patch_name.exists():+        print(f"Patch notes missing: {patch_name.name}")+        return 1++    brains_matches = list(root.glob(f"brains/v{base}{letter}__*.md"))+    if not brains_matches:+        print("Brains entry for version missing")+        return 1++    handoff_matches = list(root.glob(f"handoffs/HANDOFF_v{base}({letter}).md"))+    if not handoff_matches:+        print("Handoff file missing")+        return 1++    return 0+++if __name__ == "__main__":  # pragma: no cover+    sys.exit(main())diff --git a/tools/verifiers/Verify-UI-Contract.py b/tools/verifiers/Verify-UI-Contract.pynew file mode 100755index 0000000000000000000000000000000000000000..2b9a683232ccc0335bb7f3f266120d3576a52c98--- /dev/null+++ b/tools/verifiers/Verify-UI-Contract.py@@ -0,0 +1,33 @@+#!/usr/bin/env python3+"""Headless UI contract verification."""++from __future__ import annotations++import json+import sys+from pathlib import Path++from app.ui.main import get_ui_contract++EXPECTED_TABS = ["Overlay", "Differential", "Star Hub", "Docs"]+EXPECTED_SIDEBAR = ["Examples", "Display Mode", "Units", "Duplicate Scope", "Line Overlays"]+++def main() -> int:+    contract = get_ui_contract()+    if contract.tabs != EXPECTED_TABS:+        print(f"Tab contract mismatch: {contract.tabs} != {EXPECTED_TABS}")+        return 1+    if contract.sidebar_sections != EXPECTED_SIDEBAR:+        print("Sidebar contract mismatch")+        return 1+    version_path = Path(__file__).resolve().parents[2] / "app" / "config" / "version.json"+    payload = json.loads(version_path.read_text())+    if not payload.get("app_version"):+        print("Version badge missing app_version")+        return 1+    return 0+++if __name__ == "__main__":  # pragma: no cover+    sys.exit(main()) 
